{"cells":[{"cell_type":"markdown","source":["# Predicting Flight Delays\n##### Team: House Spark\n##### Phase IV Lead: Briana Hart, Lord Commander of Binary, Queen of the Compute, Warden of Spark, sits on the Iron Throne"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7c2edc36-8cc7-4d64-8083-c8be0544c3b7","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Team Members & Project Plan\n\n\n### Team Members of House Spark\n<style>\ntd {\n  width:240px;\n  text-align: center;\n}\n</style>\n\n<table>\n  <tr>\n    <td><img style=\"margin-top:10px\" src=\"https://raw.githubusercontent.com/brianahart/spark_flight_predictions/main/_DSC9588.jpg\" alt=\"bri\" width=\"200px\"> </td>\n    <td><img src=\"https://raw.githubusercontent.com/brianahart/spark_flight_predictions/main/image.png\" alt=\"oleg\" width=\"200px\"> </td>\n    <td><img src=\"https://raw.githubusercontent.com/brianahart/spark_flight_predictions/main/20220808_MiMsPortraits_bhs_025%202.jpeg\" alt=\"annie\" width=\"200px\"> </td>\n    <td><img src=\"https://raw.githubusercontent.com/brianahart/spark_flight_predictions/main/Screen%20Shot%202022-10-30%20at%206.32.49%20PM.jpg\" alt=\"neil\" width=\"200px\"> </td>\n    \n    \n  </tr>\n  <tr>\n    <td>Briana Hart<br>briana.hart@berkeley.edu</td>\n    <td>Oleg Ananyev<br>olegananyev@berkeley.edu</td>\n    <td>Annie Passan<br>anuradha.passan@berkeley.edu</td>\n    <td>Neil Prabhu<br>neilprabhu@berkeley.edu</td>\n  </tr>\n</table>\n\n### Phase Leader Plan\n| Task       | Leader |\n|------------|--------|\n| Phase I    | Neil   |\n| Phase II   | Oleg   |\n| Phase III  | Annie  |\n| Phase IV   | Bri    |\n| Phase V    | Neil    |\n\n### Credit Assignment \n\n| Task                             | Contributor(s)  | Date Started   | Date Completed | Hours Spent | \n|----------------------------------|-----------------|----------------|----------------|-------------|\n| Notebook Write Up                | Everyone        | 11/30/2022     | 12/04/2022     |     20      |\n| Additional EDA                   | Oleg            | 11/30/2022     | 12/03/2022     |     10      |\n| Model Pipeline Development       | Bri             | 12/01/2022     | 12/04/2022     |     15      |\n| Linear Model Experiments/GS      | Neil, Annie     | 11/30/2022     | 12/03/2022     |     10      |\n| Tree-Based Model Experiments/GS  | Annie           | 11/30/2022     | 12/03/2022     |     15      | \n| Neural Network Experiments/GS   | Neil            | 12/01/2022     | 12/03/2022     |     15      |\n| Cross Validation Experiments/GS  | Bri             | 11/30/2022     | 12/03/2022     |     10      |\n| Final Model Testing              | Bri             | 12/01/2022     | 12/03/2022     |      5      |\n| Experimentation Write Up         | Annie           | 12/02/2022     | 12/03/2022     |      3      |\n| Results Write Up                 | Bri             | 11/30/2022     | 12/04/2022     |      4      |\n| Gap Analysis                     | Oleg            | 12/04/2022     | 12/04/2022     |      2      |\n| Conclusion                       | Bri, Annie      | 12/03/2022     | 12/04/2022     |      2      |\n| Powerpoint Presentation (Short)  | Neil            | 11/30/2022     | 12/04/2022     |      4      |\n| Powerpoint Presentation (Long)   | Neil, Annie     | 11/30/2022     | 12/04/2022     |      4      |\n| 2 Minute Video Update            | Bri             | 12/04/2022     | 12/04/2022     |     20 min  |\n| Update Project Leaderboard       | Oleg            | 12/04/2022     | 12/04/2022     |      5 min  |\n| Submission                       | Oleg            | 12/04/2022     | 12/04/2022     |     15 min  |\n\nNote: GS stands for Grid Search \n\n\n### Project Plan\n\n<img src=\"https://raw.githubusercontent.com/brianahart/spark_flight_predictions/main/Screenshot%202022-12-02%20at%205.01.01%20PM.png\" alt=\"gantt\" width=\"1000\"/><br/>\n\n[Monday.com project plan](https://berkeley417683.monday.com/boards/3393322628/views/78370466)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1826cc6d-6f91-4396-8574-300e0779192d","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Other Notebook Links\n\n- <a href=\"https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/786702209422788/command/786702209422789\" target=\"_blank\"> Data Pipeline Part I<a/>\n- <a href=\"https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1325974983871287/command/1325974983871304\" target=\"_blank\"> Data Pipeline Part II<a/>\n- <a href=\"https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/844424046180181/command/917638415058265\" target=\"_blank\">EDA: Phase I (Raw Data) Q1<a/>\n- <a href=\"https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1020093804825911/command/1020093804825965\" target=\"_blank\">EDA: Phase II (Joined Data) <a/>\n- <a href=\"https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/2647101326221675/command/2647101326221746\" target=\"_blank\">EDA: Phase III (Feature Engineered Data) <a/>\n- [EDA: Phase IV](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/632558266977013/command/1215577238242060)\n- <a href=\"https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1860389250617691/command/1215577238238662\" target=\"_blank\"> Model Experimentation Without Cross Validation<a/>\n- [Model Experimentation Neural Network](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1860389250614781/command/1215577238239435)\n- [Model Experimentation with Cross Validation](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1215577238253368/command/1215577238259148)\n- [Smote Investigation](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/632558266976312/command/1215577238239427)\n- [Classification to Regression Pipeline Investigation](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1215577238255625/command/1215577238255626)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e24c802a-9321-488b-b454-bcbd10451c3b","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Abstract & Phase Summaries\n\n## Abstract\n\nAirlines delays and cancellations are a common occurrence and an unfortunate part of travel today. It can happen for a number of reasons, e.g., weather delays, equipment failures, staff shortages, etc. When it does occur - it is a significant inconvenience to passengers, airlines, and airports, and can have a significant snowball effect. The purpose of this project is to use airline, and weather data to better predict airline delays (by 15 minutes or more) using several machine learning algorithms. In the real world, this type of model could be executed an hour before take off time. The hope is that airport staff would be able to use the predictions from these models to better manage passengers and other relevant workstreams. Specifically we will be using logistic regression to develop a baseline model, and build on it with improved logistic and linear regression models, decision trees, ensemble learning methods (random forests and gradient boosted trees), and finally neural network Multilayer Perceptron (MLP) model to predict whether a flight will take off on time. To evaluate our different models we are focusing on using the F1 score and accuracy for the classification models, and then Mean Absolute Error (MAE) for the regression models. In this phase, we build out the pipeline to support the additional random forest, XG Boost and MLP models along with cross validation for each. The best performing model for Classification is Decision Tree (DT) with an F1 score of 0.774 and for Regression is Linear Regression (LR) with an MAE of 18.07. Moving forward both our models have room for improvement before going into production. \n\nNote: F1 and MAE are defined in the Modeling Metrics section below.\n\n## Phase Summaries\n\n### Phase I\n\nThe main aim of phase I was to have a better understanding of the airline and weather data that will allow us to plan for our analysis in the further phases. We settled on developing a classification model to predict whether a flight would either take off on time, be delayed, or be cancelled. Then for flights predicted to be delayed, we would feed them into a regression model to predict by how long the delay would be. In phase I we focused on exploring the flight, weather, weather station, and airport data at hand. We completed some initial EDA to give a better insight as to the quality of the datasets and the information they hold. Based on this we developed a plan for processing the data, including how to join the data together for building the models. We also decided to use a logsitic regression to develop our baseline model, and then use decision tree, Random Forest, and XG Boost to develop more sophisticated models. \n\n### Phase II\nPhase II focused on taking much of what was discussed and planned in phase I, and putting it into action into a data pipeline. Specifically we focused on building data pipeline, modeling pipeline, and running a few baseline model experiments. The data pipeline consisted of processing and cleaning the data, developing and executing a join to bring information from all 4 tables, and then some feature engineering to add in a few columns that capture information on Covid-19 and holiday trends. Specifically regarding the join, we did the join on 3 month data as a test, and then on the full time series data. From here, we developed a modeling pipeline to properly feed the data into a baseline logistic regression model. The baseline model was learned using training and validation data (i.e. from until year 2020) and tested on 2021 data, and then again using cross validation. We were able to achieve 82% accuracy in our baseline model. In phase III we will aim to expand this model to try and use it to feed into a regression model to predict how many minutes a flight will be delayed (or not) if the flight is not cancelled. Moreover, we will add in some additional features to leverage, do hyperparameter tuning, and trying out classification models (i.e. decision trees, random forest, XG Boost).  \n\n### Phase III\nWe had two main areas of focus for phase III. The first was to finalize the data that would be used in modeling and the second was to move forward in developing more advanced and accurate models than our original baseline. However, prior to focusing on these two areas, we made some changes to our joined data to include weather information for each flights' destination, and ran the full join in one go, without splitting by year (and union-ing yearly data at the end). Once this was done, we finalized the dataset by doing some extra cleaning on the data and adding in extra predictive features to enhance our model building; and we moved forward on model development. Specifically we ran four different types of models which includes Linear Regression, Logistic Regression, Decision Tree Regression, and Decision Tree classification. For each model, we ran it on 1% sample size data sets, 10% sample size data sets in addition to oversampling, undersampling, and none sampling. We also adjusted the parameters for each model such as the iterations, regularization, elastic net parameters, depth, bins, and impurity. We found that the best models were the decision tree models as they outperformed the Linear and Logistic models by a sizeable margin. You can find the specific results below in the Modeling - Results section. In developing our models we did find that over and under sampling (to account for our imbalanced dataset where ~80% of the labels are not delayed or cancelled) decreased model performance, so in phase IV we will try to investigate that further and look into using different sampling methods (e.g. SMOTE). We will also look into experimenting with ensemble learning methods; specifically random forest and XG-Boost. \n\n\n### Phase IV\n\nFor phase 4, our primary area of focus was building out the pipeline for the more complex ML models, MLP neutral network, Random Forest, and Gradient Boosted Trees.  We each new model with 1% and 10% sample size datasets to ensure they are functional. After determining the models are functioning as intended, we adjusted the parameters for each model such as the iterations, regularization, elastic net parameters, depth, bins, and impurity. We also built out a cross validation function and SMOTE function to further evaluate and refine our models with enhanced sampling. After determining the full functionality of our models and cross validation and SMOTE functions, we ran our models against the full datasets with data through 2021. However, we came across resource issues in the final week of the project which made the additional functionality challenging to implement. For the majority of time we were only able to leverage four workers which made iteration challenging. Because of this, we chose to stop using the SMOTE sampling for our models. With regards to model performance, we found that the best performing model was Decision Trees for classification and Linear Regression for regression. The evaluation metrics for each can be found in the \"Modeling\" section below."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"059068ad-103d-4103-b142-cae4fdf15cbf","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Introduction\n\n## Project Objective\n\nDevelop a classification model to predict whether a flight will either leave on time, be delayed, or be cancelled, and then use results of the classification model to run a regression to predict by how many minutes a flight will be delayed. A delay is defined as a flight leaving 15 minutes or more after the scheduled departure time. If a flight leaves less than 15 minutes after depature time, it will be considered to be leaving on time. The key audience of this project will be airport staff. \n\nWith the audience in mind, we will be analyzing the data with the key question in mind of \"can we accurately predict a flight  delay?\" This will potentially branch out into additional questions such as (1) which factors result in delays, (2) which of these factors are within the airport staff's control that they can get ahead of, and (3) which of these factors are not within their control but can work with associated/involved parties to potentially limit impact."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dffe821e-d076-4ecc-aae8-614716118029","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Notebook Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2e7e856d-4344-4996-ac25-4088b6fc8ec7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%pip install timezonefinder\n%pip install tzfpy"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"f641d51a-88a4-46ab-bb0c-5686243a68b0","inputWidgets":{},"title":"Install Modules"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Python interpreter will be restarted.\nCollecting timezonefinder\n  Downloading timezonefinder-6.1.8.tar.gz (45.9 MB)\n  Installing build dependencies: started\n  Installing build dependencies: finished with status 'done'\n  Getting requirements to build wheel: started\n  Getting requirements to build wheel: finished with status 'done'\n    Preparing wheel metadata: started\n    Preparing wheel metadata: finished with status 'done'\nCollecting h3<4,>=3.7.6\n  Downloading h3-3.7.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\nCollecting cffi<2,>=1.15.1\n  Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB)\nCollecting setuptools>=65.5\n  Using cached setuptools-65.6.3-py3-none-any.whl (1.2 MB)\nRequirement already satisfied: numpy<2,>=1.18 in /databricks/python3/lib/python3.9/site-packages (from timezonefinder) (1.20.3)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.9/site-packages (from cffi<2,>=1.15.1->timezonefinder) (2.20)\nBuilding wheels for collected packages: timezonefinder\n  Building wheel for timezonefinder (PEP 517): started\n  Building wheel for timezonefinder (PEP 517): finished with status 'done'\n  Created wheel for timezonefinder: filename=timezonefinder-6.1.8-cp39-cp39-manylinux_2_31_x86_64.whl size=45919978 sha256=1507861545b77f8959343ab32ca7121d39b8efab5d624da71626025d843dfbd0\n  Stored in directory: /home/spark-9d6120c4-cc3b-4f11-88f0-ea/.cache/pip/wheels/e6/43/c7/911bd3351ddf33fe74a51d98c51a2e6563ab046b77f37eeef7\nSuccessfully built timezonefinder\nInstalling collected packages: setuptools, h3, cffi, timezonefinder\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 58.0.4\n    Not uninstalling setuptools at /usr/local/lib/python3.9/dist-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-9d6120c4-cc3b-4f11-88f0-ea02fb5a4979\n    Can't uninstall 'setuptools'. No files were found to uninstall.\n  Attempting uninstall: cffi\n    Found existing installation: cffi 1.14.6\n    Not uninstalling cffi at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-9d6120c4-cc3b-4f11-88f0-ea02fb5a4979\n    Can't uninstall 'cffi'. No files were found to uninstall.\nSuccessfully installed cffi-1.15.1 h3-3.7.6 setuptools-65.6.3 timezonefinder-6.1.8\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting tzfpy\n  Downloading tzfpy-0.11.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.0 MB)\nInstalling collected packages: tzfpy\nSuccessfully installed tzfpy-0.11.1\nPython interpreter will be restarted.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Python interpreter will be restarted.\nCollecting timezonefinder\n  Downloading timezonefinder-6.1.8.tar.gz (45.9 MB)\n  Installing build dependencies: started\n  Installing build dependencies: finished with status 'done'\n  Getting requirements to build wheel: started\n  Getting requirements to build wheel: finished with status 'done'\n    Preparing wheel metadata: started\n    Preparing wheel metadata: finished with status 'done'\nCollecting h3<4,>=3.7.6\n  Downloading h3-3.7.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\nCollecting cffi<2,>=1.15.1\n  Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB)\nCollecting setuptools>=65.5\n  Using cached setuptools-65.6.3-py3-none-any.whl (1.2 MB)\nRequirement already satisfied: numpy<2,>=1.18 in /databricks/python3/lib/python3.9/site-packages (from timezonefinder) (1.20.3)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.9/site-packages (from cffi<2,>=1.15.1->timezonefinder) (2.20)\nBuilding wheels for collected packages: timezonefinder\n  Building wheel for timezonefinder (PEP 517): started\n  Building wheel for timezonefinder (PEP 517): finished with status 'done'\n  Created wheel for timezonefinder: filename=timezonefinder-6.1.8-cp39-cp39-manylinux_2_31_x86_64.whl size=45919978 sha256=1507861545b77f8959343ab32ca7121d39b8efab5d624da71626025d843dfbd0\n  Stored in directory: /home/spark-9d6120c4-cc3b-4f11-88f0-ea/.cache/pip/wheels/e6/43/c7/911bd3351ddf33fe74a51d98c51a2e6563ab046b77f37eeef7\nSuccessfully built timezonefinder\nInstalling collected packages: setuptools, h3, cffi, timezonefinder\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 58.0.4\n    Not uninstalling setuptools at /usr/local/lib/python3.9/dist-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-9d6120c4-cc3b-4f11-88f0-ea02fb5a4979\n    Can't uninstall 'setuptools'. No files were found to uninstall.\n  Attempting uninstall: cffi\n    Found existing installation: cffi 1.14.6\n    Not uninstalling cffi at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-9d6120c4-cc3b-4f11-88f0-ea02fb5a4979\n    Can't uninstall 'cffi'. No files were found to uninstall.\nSuccessfully installed cffi-1.15.1 h3-3.7.6 setuptools-65.6.3 timezonefinder-6.1.8\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting tzfpy\n  Downloading tzfpy-0.11.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.0 MB)\nInstalling collected packages: tzfpy\nSuccessfully installed tzfpy-0.11.1\nPython interpreter will be restarted.\n"]}}],"execution_count":0},{"cell_type":"code","source":["# General \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport sys\nfrom statistics import mean\nimport itertools\n\n# PySpark \nfrom pyspark.sql.functions import col,isnan,when,count\nfrom pyspark.sql.functions import regexp_replace\n\n# SQL Functions\nfrom pyspark.sql import functions as f\nfrom pyspark.sql.functions import monotonically_increasing_id, to_timestamp, to_utc_timestamp, to_date\nfrom pyspark.sql.functions import isnan, when, count, col, isnull, percent_rank, first, dense_rank\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, NullType, ShortType, DateType, BooleanType, BinaryType, FloatType, DecimalType\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.window import Window\nfrom pyspark.streaming import StreamingContext\nfrom pyspark.sql import Row\nfrom functools import reduce\nfrom pyspark.sql.functions import rand,col,when,concat,substring,lit,udf,lower,sum as ps_sum,count as ps_count,row_number\nfrom pyspark.sql.window import *\nfrom pyspark.sql import DataFrame\nfrom pyspark.ml.stat import Correlation\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.linalg import DenseMatrix, Vectors\nfrom pyspark.sql.functions import row_number\n\n# ML\nfrom pyspark.ml.stat import Correlation\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler, OneHotEncoder\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Misc \nfrom pandas.tseries.holiday import USFederalHolidayCalendar as calendar\nfrom timezonefinder import TimezoneFinder\nfrom tzfpy import get_tz"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"86437001-ff1b-4d65-a465-d8f67a4b7beb","inputWidgets":{},"title":"Import Modules"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":{"text/plain":"","application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"},"removedWidgets":[],"addedWidgets":{},"metadata":{"kernelSessionId":"a612b1f0-15b79f89a01517b2f07da152"},"type":"mimeBundle","arguments":{}}},"output_type":"display_data","data":{"text/plain":"","application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"}}],"execution_count":0},{"cell_type":"code","source":["# Display and define where mids-w261 is located\ndata_BASE_DIR = \"dbfs:/mnt/mids-w261/\"\n# display(dbutils.fs.ls(f\"{data_BASE_DIR}\"))\n\n# Inspect the Mount's Final Project folder \ndata_BASE_DIR = \"dbfs:/mnt/mids-w261/datasets_final_project_2022/\"\n# display(dbutils.fs.ls(f\"{data_BASE_DIR}\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"8ea5cf50-f7c0-415d-9894-734ac509088c","inputWidgets":{},"title":"Locate Data"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["blob_container = \"housestark\" # The name of your container created in https://portal.azure.com\nstorage_account = \"neilp\" # The name of your Storage account created in https://portal.azure.com\nsecret_scope = \"w261_s1g4\" # The name of the scope created in your local computer using the Databricks CLI\nsecret_key = \"w261_s1g4_key\" # The name of the secret key created in your local computer using the Databricks CLI \nblob_url = f\"wasbs://{blob_container}@{storage_account}.blob.core.windows.net\"\nmount_path = \"/mnt/mids-w261\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"bd7ce097-a7ef-4750-80fa-9c54dc6a568a","inputWidgets":{},"title":"Blob info"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.conf.set(\n  f\"fs.azure.sas.{blob_container}.{storage_account}.blob.core.windows.net\",\n  dbutils.secrets.get(scope = secret_scope, key = secret_key)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"18f526af-51c0-4652-96a3-ab8912d99f7a","inputWidgets":{},"title":"Blob info (II)"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Data Lineage"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"831f7002-bb77-478b-9632-2bae116c7fdc","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Data Description\n\nThe below table states the four different datasets that will be used, their respective sizes, and where they were obtained.\n\n| Dataset    | Size (Compressed)    | Source |\n|------------|--------|--------|\n| Airline    | 2.8 GB | [United States Department of Transportation](https://www.transtats.bts.gov/homepage.asp)  |\n| Weather    | 33 GB  | [National Oceanic and Atmospheric Observatory Repository](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ncdc:C00679)    |\n| Stations   | 53 MB    | Provided by W261 instructors  |\n| Airport    | 5.6 MB | [Third-party Open Source Airport Data](https://davidmegginson.github.io/ourairports-data/airports.csv)   |\n\nThe following four subsections will provide initial insights on what information is included in the datasets to help inform the actual project task the team will work on.\n\n**Airline Dataset**\n- 74,177,433 rows \n- 108 columns\n\nThe airlines dataset provides flight information in the United States. In particular it gives information on whether a flight was on time, delayed, cancelled, or diverted (more details in the information highlights section below). This dataset will be key in developing any model on flight delays, as it contains the data labels on whether or not a flight was on time, delayed, or cancelled. It is likely that the model will not consider flights that have been diverted because normally a flight diversion happens after a flight has taken off, and to predict a flight delay, only variables that can be known before a flight has taken off can be considered. This also means that in addition to variables on diverted flights, variables that speak to things that have happened when a flight lands will not be useful. The only variables that could be of use is whether or not a flight has been delayed, cancelled, and delayed by how many minutes, as these will be used in generating the appropriate labels the model will try to predict. \n\n**Weather Dataset**\n- 898,983,399 rows \n- 123 columns\n\nThis dataset contains weather readings for weather stations across the United States (for initial EDA, a subset of first quarter of 2015 was used). It is well known that intense weather conditions such as high winds or heavy snow can make it difficult for pilots to navigate a plane safely, which is why weather related delays are common. Weather delays is a category in the airlines dataset, further supporting the idea that weather related information could be useful in predicting flight delays and cancellations. \n\n**Weather Stations Dataset**\n- 5,004,169 rows\n- 12 columns\n\nThe station dataset gives information collected at weather stations located closeby airport. It gives information on where the station is located and will be useful in joining with the weather dataset to get the relevant weather readings for each airport at the specific moment in time. Further information on joining tables is in the joins section. There are no null values. \n\n**Airport Dataset**\n- 73,043 rows\n- 18 columns\n\nThe airport dataset gives information on global airports. This outside dataset will provide a common key such as GPS code or longitude/latitude which will allow us to join airport data with weather data. The dataset is a third-party open-source data set and was last updated November 13th, 2022.\n\nFurther EDA that was done to help inform our model experimentation is in the section following *Data Lineage*."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"25b23cce-075b-4c8a-8edf-ee7cc44c8d0c","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Data Pipeline\n\nThe below image shows the workflow of our full pipeline:\n<img src=\"/files/shared_uploads/briana.hart@berkeley.edu/Algorithm_flowchart_example__1_.jpeg\" width=\"1000px\" height=\"100px\"> \n\nThe data pipeline will cover everything until modeling, while the modeling pipeline will cover the steps after. \n\nAll of the data processing up until (and including) the execution of the final join was done in the following notebook: <a href=\"https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/786702209422788/command/786702209422789\" target=\"_blank\"> Data Pipeline Part I<a/>.\n  \nAll of cleaning and feature engineering done after the join was done in another notebook: <a href=\"https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1325974983871287/command/1325974983871304\" target=\"_blank\"> Data Pipeline Part II<a/>. \n\n## Data Pipeline Part I (Until Join Execution)\nRefer to the following notebook: <a href=\"https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/786702209422788/command/786702209422789\" target=\"_blank\"> Data Pipeline Part I<a/>\n  \nThis first notebook lays out the process for the following:\n1. Notebook Setup\n2. Reading in the full airlines, weather, weather stations, and airport dataset\n3. Airlines data cleaning + checkpoint to blob storage \n4. Weather data cleaning + checkpoint to blob storage\n5. Other datasets data cleaning \n6. Execution of the joins (airport data, destination weather, final join) + checkpoint to blob storage \n\nThe above steps refer to the most updated join. In phase II, we tried first to run a join on all of the data but the process was not optimized well and went on for 24+ hours. We then split up the \nAs implied above, we decided to break up the final data set in to sizable chunks by year for processing, checkpoint the cleaned data to blob storage for each year, conduct the join for each year, and then at the end bring them together with a union. This worked out relatively well timewise as it took ~ 25 minutes to execute the whole thing. But in phase III we realized that we wanted to also include some extra columns on destination weather, so we re-ran our final/current join that takes into account the destination weather 2 hours after the original flight departure time because we assume in reality, this is the most accurate weather prediction at the hourly level we would have access to. The origin weather takes into account the weather read at 1 hour before original departure time because that is when the model would ideally be run.\n  \nThe following sections will go through the different steps in the first part of the data pipeline.\n\n#### Read in Full Data\nThe first part of the data pipeline was to read in the data. This is shown below for both 3 month and the full dataset. For the full dataset, since we have to split by year, we added in a year column to the weather and airlines datasets. \n  \n#### Feature Selection\nAfter an initial glance at the data, we knew there would be certain columns we would not be using. As such we decided to remove the columns in question to optimize data processing. In the airlines dataset, we removed columns relating to flight arrivals and some flight summary data (e.g. AIR_TIME) because in reality, we would not have access to data from the \"future\". In case there are scenarios where an aircraft arrives late and thus delays the flight about to take off, the LATE_AIRCRAFT_DELAY variable takes care of highlighting this. We also removed columns relating to flight diversions. Flight diversions can happen before take off or while the plane is in air, as such we are not including this in our model objective. In the weather dataset, we removed daily, monthly, and short duration columns because the hourly weather data will be more relevant for our context.\n\n#### Data Checks and Cleaning\n  \n**Airlines Dataset**\n\nWe reviewed the airlines dataset (3 months) to get a better understanding of the data. Starting with null values, 'TAIL_NUM' could be null because of military planes as they are typically unmarked and it is an insignificant fraction of the three month sample dataset. 'DEP_DELAY' is null for canceled flights and is ~3% of the 3 month sample dataset, this means if the flight wasn’t canceled, there will be a value for 'DEP_DELAY'. Checking the reverse - there are instances where 'DEP_DELAY' has a value and a flight is also canceled ('CANCELED' = 1). This amount to ~2,292 flights in the same dataset. Reviewing the dataset, it looks like these flights had a planned departure however they were subsequently canceled during taxi out or slightly after take off because there is no arrival time. We can leave these as is. The cancel codes vary from A-Carrier Caused / B-Weather / C-National Aviation System / D-Security. \n\nReviewing additional columns such as 'ORIGIN' (airport code), 'OP_UNIQUE_CARRIER' (airline), 'DEST_STATE_NM' (state name), the values within these columns are acceptable and clean with no visible typos. Further reviewing the yearly datasets, they are all consistent in terms of datatypes and variable names. Only two column to call out specifically is 'FL_DATE' (flight date) - it is in ‘YYYY-MM-DD’ string format, it needs to be converted to date time for usability. In addition, 'YEAR' appears to be a specific variable from the 3 month sample dataset as it is not available in the full year datasets. \n\nTo get this data ready for the full join, we did need to reformat the flight date and times to a UTC timestamp, and another flight date and hour timestamp rounded to the most recent hour (e.g. 10:20am and 10:40am would both get rounded to the 10:00am). The rounded timestamp will serve as one of the keys in joining the airlines and weather dataset. The weather information is given at an hourly basis, which is why a rounded timestamp is necessary. \n  \n**Weather Dataset**\n\nWe did some further investigation on the 3 month sample weather dataset. Two main issues of this dataset is the high prevelance of null values and the fact that many stations have weather data from multiple different sources. When we removed certain columns in the feature selection (above), it removed many columns with majority null values. And for the columns we keep with null values, we will impute values based on time and location in the feature engineering phase after the join and data splits. To address the different sources, we decided to pick source 7 which brings in weather information from the United States Air Force (USAF) and Automated Surface/Weather Oberving Systems/Automated Weather Observing System (ASOS/AWOS - both regulated by the Federal Aviation Administration) on an hourly basis. \n\nFurthermore, we needed to convert the DATE column to a UTC timestamp to be able to join with the airlines data. The function below takes care of this. Finally, there are many values in the dataset that inidicate missing value (e.g. '999'). In the early stages of the data pipeline development we decided to remove these values, but it heavily slowed down the process, as such we will change them to nulls after the final join. \n  \n**Weather Stations and Airports Dataset**\n\nBoth of these datasets have been brought in because they possess information to join the airlines and weather dataset. The airports dataset has information on airport longitude and latitude which is used in helping convert the flight times to their proper timezone to be then converted to UTC. The weather stations dataset has information relating to station IDs near airports which will be used in helping us find the closest weather station ID to the airport to help join the airlines and weather dataset. The columns we will be using are clean and no duplicates were found in both datasets. The irrelevant columns will be deleted prior to the full join. \n  \nAfter cleaning through the different datasets, we removed unnecessary raw data tables from memory to help speed up the join process. \n\n#### Join \n\n**Join Process**\n\nThis full process was conducted with the following cluster size: **14GB and 4 Cores**.\n    \nSteps in joining our clean data:\n1. Join the airlines and airports data on the corresponding IATA code\n2. Using information from the airports dataset, apply the convert_to_utc function to get the UTC timestamps for the airlines data\n3. Create a dataframe of the closest weather stations to airports based on the distance_to_neighbor column in the weather stations dataset\n4. Run the transform_to_ICAO function to get the ICAO code for the airlines_airports dataset\n5. Join the closest stations and airlines_airports dataset on the ICAO code\n6. Drop columns that are no longer needed in this process\n7. Conduct the final join for airlines_airports and the weather station on the corresponding UTC timestamp (to the hour) and closest weather station\n8. Remove columns no longer needed post final join \n9. Save to parquet\n\nThe idea of checkpointing our clean data to the blob storage as well as after the join execution was to help Spark split up the calculations it is doing, as it will only compute when called to do an action (such as write to parquet).\n  \nWe include below the times and shape of the datasets in conducting the final/current join.\n\n**Time to Read and Checkpoint to Blob Storage:**\n\n| DataFrame           | Write Time            |\n|---------------------|-----------------------|\n|Cleaned Airlines     | 6.03 minutes          |\n|Cleaned Weather      | 57.43 minutes         |\n|Full Joined Dataset  | 11.52 minutes         |\n\n### Shape of DataFrames Table:\n| DataFrame                   | Rows           | Columns         |\n|-----------------------------|----------------|-----------------|\n|Airlines Raw                 | 74,177,433     | 108             |\n|Airlines Cleaned             | 42,430,592     | 51              |\n|Weather Raw                  | 898,983,399    | 123             |\n|Weather Cleaned              | 223,810,507    | 35              |\n|Weather Stations Raw + Clean | 5,004,169      | 12              |\n|Airports Raw + Clean         | 74,043         | 18              |\n|Full Joined Dataset          | 42,430,592     | 95              |\n  \n  \n## Further Cleaning and Feature Engineering\n  \nThis is the second part of the data pipeline, and the relevant code can be found here: <a href=\"https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1325974983871287/command/1325974983871304\" target=\"_blank\"> Data Pipeline Part II<a/>.\n  \nAt this point, we have completed joining the full data. In this moment we would like to do some further cleaning and feature engineering. Some of the cleaning was postponed to be done after the join to speed up the join process.\n\nTo address the final cleaning and feature engineering, we will follow the below steps:\n1. Read in the fully joined dataset\n2. Remove columns we know we will not be using\n3. Convert columns to proper data types \n4. Trim any trailing or leading spaces in string columns\n5. Add in extra features \n6. Conduct extra cleaning \n7. Inspect and write to parquet\n\nIt is important to note that the above workstream for cleaning and feature engineering does not include all of the cleaning and predictive features that need to be done/added. We plan to impute nulls for weather data to grab the latest known information going back 3 hours maximum, and we wanted to add in features that take into account a 24 hour window. Originally we had done this for 90 days, but during model experimentation we realized it might have too stale data to have much predictive power. Because it will not be known until the data split which rows will be a trained/validated/tested, we cannot impute these nulls and add in these predictive features in this stage because a row can have information based on previous rows that may not be in its partition of the data (for train, test, etc.). As such, we have chosen to move these two tasks to the modeling pipeline once the data is properly split, to prevent any potential data leakage that could impact our models. \n\nWe have written a function that can be run on each data split before training a model to take care of these two tasks. This will be run in the modeling pipeline. \n- Impute null values for certain weather columns based on last known information in the past 3 hours\n- Further null imputations based on remaining null values \n- Calculate the following numerical variables:\n  - Number of flights in the past 24 hours for a plane (aka tail number)\n  - Number of flight delays in the past 24 hours for a plane\n  - Number of flight cancellations in the past 24 hours for a plane\n  - Percentage of flight delays in the past 24 hours for a plane\n  - Percentage of flight cancellations in the past 24 hours for a plane\n\n#### Added Features\nIn particular the following features were added to our dataset:\n\n**Year**\n- This is the year of the scheduled departure (UTC) for each flight. \n- We added this in as a categorical variable to help in wrangling the data in the modeling pipeline, but also to help capture any trends on a yearly basis.\n\n**dep_delay_15**\n- Binary variable on whether a flight was delayed more than 15 minutes in accordance with our definition that a delay is defined by 15 or more minutes. This is used to help data wrangling and is not included in the modeling. \n- 0 = on time, early, or less than 15 minutes late\n- 1 = delayed by more than 15 minutes\n\n**Classification Label**\n- This will be the label for our classification models; it is an ordinal variable as well. \n- 0 = on time, early, or less than 15 minutes late\n- 1 = delayed by more than 15 minutes\n- 2 = cancelled \n\n**holiday**\n- Binary variable on whether a date is an American Federal Holiday. We added in this feature based on the notion that on holidays more people tend to travel out of town, and can time-wise afford to travel out of town due to the holiday. This variable is used to build the below variable and is not in the modeling.  \n- 0 = not holiday \n- 1 = holiday\n\n**holiday_in2DayRange**\n- Binary variable on whether a date is an American Federal Holiday, along with the 2 days prior and after. This is to support the above variable (only this one used in modeling) with the notion that usually +/- 2 days around a holiday is when people are flying to/back from different destinations - especially during long weekends. We think this variable probably accounts for holiday related traffic more than the holiday variable because it takes into account the time to travel to and back on holiday. \n- 0 = not holiday or +/- 2 days\n- 1 = holiday or +/- 2 days\n\n**C19**\n- This is an ordinal variable tying in the story of what was happening in the passenger traffic portion of the aviation industry. \n- Values:\n  - Before 2020-01-17: *Score = 0*\n    - At this point COVID-19 does not exist or it has not affected the aviation industry yet.\n  - 2020-01-17 - 2020-03-14: *Score = 1*\n    - In this time period passengers in the United States begin to get screened for COVID-19. \n  - 2020-03-15 - 2020-08-05: *Score = 4*\n    - States begin shutdowns, airline traffic drops heavily, and international borders close. \n  - 2020-08-06 - 2021-04-01: *Score = 3*\n    - On 6 August 2020, the U.S. Department of State lifted a Level 4 global health travel advisory issued on 19 March which advised American citizens to avoid all international travel\n    - Some travel resumes for passengers who can provide negative COVID-19 tests.\n  - 2021-04-02 - TODAY: *Score = 2*\n    - Even though COVID-19 is still a pandemic, vaccinated passengers can travel with ease, but air traffic is not what it was up to in 2019. \n- More information on the reasoning behind these values can be found in the Data Pipeline Part II notebook (link above). \n- Sources used to inform our above decisions:\n  - <a href=\"https://www.health.harvard.edu/blog/is-the-covid-19-pandemic-over-or-not-202210262839\" target=\"_blank\"> Harvard Health <a/> \n  - <a href=\"https://www.cdc.gov/museum/timeline/covid19.html\" target=\"_blank\"> CDC <a/>\n  - <a href=\"https://transport.ec.europa.eu/system/files/2021-12/Special%20report%20on%20COVID-19%20impact%20on%20the%20US%20and%20European%20ANS%20systems.pdf\" target=\"_blank\"> Federal Aviation Administration, European Commission, EuroControl<a/>\n  - <a href=\"https://en.wikipedia.org/wiki/Travel_during_the_COVID-19_pandemic#Traveling_to_vaccinated_venues_that_mandate_COVID-19_vaccines_to_tourist_or/add_staff\"> Wikipedia <a/>\n  \n  \nAt this point in the data pipeline, our dataset is cleaned, joined, and feature engineered as best can be done before splitting the data for model development. The data pipeline ends here and further wrangling will proceed in the modeling pipeline."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"46e2c34f-0d44-4bee-9128-bc27bf4fb442","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Data Dictionary \n\nPlease find below a list of the different variables in our final dataset for modeling. Variables that have an asterix before them were incorporated in our model experimentation. \n\n**Data Dictionary** \n|Feature Family | Variable | Data Type | Definition |\n|---------------|----------|-----------|------------|\n| Classification label | \\*label | integer | classification label; 0 = on time/early/delayed by less than 15 minutes, 1 = delayed by 15+ minutes\n| Regression Target Variable | \\*DEP_DELAY_NEW | integer | regression label; number of minutes flight is delayed \n| Time related | local_timestamp |timestamp | local flight time\n| Time related | timezone | string | local flight timezone\n| Time related | scheduled_departure_UTC | timestamp | flight time in UTC\n| Time related | rounded_depTimestamp | timestamp | flight time in UTC rounded to hour\n| Time related | \\*Year | integer | year of flight date \n| Time related | \\*QUARTER | integer | quarter of flight date\n| Time related | \\*MONTH | integer | month of flight date\n| Time related | \\*DAY_OF_MONTH | integer | day of month of flight date\n| Time related | \\*DAY_OF_WEEK | integer | day of week of flight date\n| Time related | \\*DEP_TIME_BLK | string |scheduled depature time (hourly blocks)\n| Flight information | \\*OP_UNIQUE_CARRIER | string | flight airline; identifier assigned by US Department of Transportation for airlines\n| Flight information | \\*TAIL_NUM | string | identifier for specific aircraft\n| Flight information | \\*OP_CARRIER_FL_NUM | string | flight number\n| Flight information | \\*DISTANCE | integer | planned flight distance\n| Flight information | DISTANCE_GROUP | integer | groups of flight distances (intervals of 250 miles); ordinal\n| Plane History | no_delays_last1d | long | number of flights delayed in past 24 hours per tail number\n| Plane History | no_cancellation_last1d | long | number of flights cancelled in past 24 hours per tail number\n| Plane History | count_flights_last1d | long | number of flights scheduled in past 24 hours per tail number\n| Plane History | \\*perc_delays_last1d | double | percentage of flights delayed in past 24 hours per tail number\n| Plane History | \\*perc_cancellation_last1d | double | percentage of flights cancelled in past 24 hours per tail number\n| Origin Information | \\*ORIGIN_AIRPORT_ID | string | origin airport ID; assigned by US Department of Transportation \n| Origin Information | ORIGIN | string | origin 3 letter airport code\n| Origin Information | ORIGIN_CITY_NAME | string | origin city\n| Origin Information | ORIGIN_STATE_ABR | string | origin state\n| Origin Information | \\*elevation_ft | integer | origin airport elevation\n| Origin Information | \\*type | string | airport size (small, medium, large)\n| Destination Information | \\*DEST_AIRPORT_ID | string | destination airport ID; assigned by US Department of Transportation \n| Destination Information | DEST | string | destination 3 letter airport code \n| Destination Information | DEST_CITY_NAME | string | destination city \n| Destination Information | DEST_STATE_ABR | string | destination state\n| Holiday | holiday | integer | whether flight date is a federal holiday\n| Holiday | \\*holiday_in2DayRange | long | whether flight date is a federal holiday +/- 2 days\n| Covid-19 | \\*C19 | integer | ordinal variable referring to impact COVID-19 was having on aviation; further information <a href=\"https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1325974983871287/command/1325974983871304\" target=\"_blank\">here<a/>\n| Origin Weather | \\*origin_HourlyAltimeterSetting | float | hourly pressure (hectopascals) value which altimeter is set to indicate altitude relative to mean sea level of aircraft on the ground at location for which value was determined; at origin\n| Origin Weather | \\*origin_HourlyDewPointTemperature | integer | hourly temperature (Celsius) which a given parcel of air must be cooled at constant pressure and water vapor content in order for saturation to occur; at origin\n| Origin Weather | \\*origin_HourlyDryBulbTemperature | integer | hourly temperature (Celsius) and is another standard way to measure air temperature; at origin\n| Origin Weather | \\*origin_HourlyPrecipitation | float | hourly rain quantity measured in millimeters; at orgin \n| Origin Weather | \\*origin_HourlyPressureChange | float | hourl number indicating air pressure change; measured in hectopascals; at origin\n| Origin Weather | \\*origin_HourlyPressureTendency | integer | hourly air pressure measured in hectopascals; at origin\n| Origin Weather | \\*origin_HourlyRelativeHumidity | integer | the amount of water vapour present in air expressed as a percentage of the amount needed for saturation at the same temperature; at origin\n| Origin Weather | \\*origin_HourlySkyConditions | string | hourly combination of codes which denote tany specific conditions in sky (e.g. SCT:04 45 SCT:04 190); at origin\n| Origin Weather | \\*origin_HourlySkyConditions_SCT_cnt | integer | number of scattered cloud layers; at origin \n| Origin Weather | \\*origin_HourlySkyConditions_OVC_cnt | integer | number of overcast cloud layers; at origin  \n| Origin Weather | \\*origin_HourlySkyConditions_FEW_cnt | integer | number of 'few' cloud layers; at origin \n| Origin Weather | \\*origin_HourlySkyConditions_BKN_cnt | integer | number of broken cloud layers; at origin \n| Origin Weather | \\*origin_HourlySkyConditions_VV_cnt | integer | number of vertical visibility cloud layers; at origin \n| Origin Weather | \\*origin_HourlySkyConditions_SKC_cnt | integer | number of clear cloud layers; at origin  \n| Origin Weather | \\*origin_HourlySkyConditions_CLR_cnt | integer | number of clear cloud layers; at origin \n| Origin Weather | \\*origin_HourlySeaLevelPressure | float | hourly air pressure (hectopascals) relative to mean sea level; at origin\n| Origin Weather | \\*origin_HourlyStationPressure | float | hourly atmospheric pressure at a weather station (hectopascals); at origin \n| Origin Weather | \\*origin_HourlyVisibility | float | hourly horizontal distance at which an object can be seen/identified; at origin \n| Origin Weather | \\*origin_HourlyWetBulbTemperature | integer | hourly average wet bulb temperature (Celsius); at origin \n| Origin Weather | \\*origin_HourlyWindDirection | integer | hourly angle measured in blockwise direction between true north and direction from which the wind is blowing; at origin \n| Origin Weather | \\*origin_HourlyWindGustSpeed | integer | hourly rate of speed of wind gust measured in meters/second; at origin \n| Origin Weather | \\*origin_HourlyWindSpeed | integer | hourly rate of speed of wind, measured in meters/secondl at origin \n| Origin Weather | \\*dest_HourlyAltimeterSetting | float | hourly pressure value (hectopascals) which altimeter is set to indicate altitude relative to mean sea level of aircraft on the ground at location for which value was determined; at destination\n| Destination Weather | \\*dest_HourlyDewPointTemperature | integer | hourly temperature (Celsius) which a given parcel of air must be cooled at constant pressure and water vapor content in order for saturation to occur; at destination\n| Destination Weather | \\*dest_HourlyDryBulbTemperature | integer | hourly temperature (Celsius) and is another standard way to measure air temperature; at destination \n| Destination Weather | \\*dest_HourlyPrecipitation | float | hourly rain quantity measured in millimeters; at destination \n| Destination Weather | \\*dest_HourlyPressureChange | float | hourly number indicating air pressure change; at destination \n| Destination Weather | \\*dest_HourlyPressureTendency | integer | hourly air pressure measured in hectopascals; at destination\n| Destination Weather | \\*dest_HourlyRelativeHumidity | integer | the amount of water vapour present in air expressed as a percentage of the amount needed for saturation at the same temperature; at destination\n| Destination Weather | \\*dest_HourlySkyConditions | string | hourly combination of codes which denote tany specific conditions in sky (e.g. SCT:04 45 SCT:04 190); at destination\n| Destination Weather | \\*dest_HourlySkyConditions_SCT_cnt | integer | number of scattered cloud layers; at destination \n| Destination Weather | \\*dest_HourlySkyConditions_OVC_cnt | integer | number of overcaset cloud layers; at destination \n| Destination Weather | \\*dest_HourlySkyConditions_FEW_cnt | integer | number of 'few' cloud layers; at destination \n| Destination Weather | \\*dest_HourlySkyConditions_BKN_cnt | integer | number of broken cloud layers; at destination \n| Destination Weather | \\*dest_HourlySkyConditions_VV_cnt | integer | number of vertical visibility cloud layers; at destination \n| Destination Weather | \\*dest_HourlySkyConditions_SKC_cnt | integer | number of clear cloud layers; at destination \n| Destination Weather | \\*dest_HourlySkyConditions_CLR_cnt | integer | number of clear cloud layers; at destination \n| Destination Weather | \\*dest_HourlySeaLevelPressure | float | hourly air pressure (hectopascals) relative to mean sea level; at destination\n| Destination Weather | \\*dest_HourlyStationPressure | float | hourly atmospheric pressure at a weather station (hectopascals); at destination\n| Destination Weather | \\*dest_HourlyVisibility | float | hourly horizontal distance at which an object can be seen/identified; at destination \n| Destination Weather | \\*dest_HourlyWetBulbTemperature | integer | hourly average wet bulb temperature (Celsius); at destination \n| Destination Weather | \\*dest_HourlyWindDirection | integer | hourly angle measured in blockwise direction between true north and direction from which the wind is blowing; at destination \n| Destination Weather | \\*dest_HourlyWindGustSpeed | integer | hourly rate of speed of wind gust measured in meters/second; at destination \n| Destination Weather | \\*dest_HourlyWindSpeed | integer | hourly rate of speed of wind, measured in meters/secondl at destination  \n| Other | \\*dep_delay_15 | integer | binary variable on whether a flight is 15+ minutes late (1) or not (0)\n| Other | CANCELLED | integer | binary of whether flight is cancelled (1) or not (0)\n| Other | CARRIER_DELAY | integer | binary variable on whether flight delayed because of airline (1) or not (0)\n| Other | WEATHER_DELAY | integer | binary variable on whether flight delayed because of weather (1) or not (0)\n| Other | NAS_DELAY | integer | binary variable on whether flight delayed due to National Air System (1) or not (0)\n| Other | SECURITY_DELAY | integer | binary variable on whether flight delayed due to security reasons (1) or not (0)\n| Other | LATE_AIRCRAFT_DELAY | integer | binary variable on whether flight delayed due to late flight in same gate (1) or not (0)\n| Time related | scheduled_departure_UTC_minus_1hr | timestamp | timestamp used to match origin weather at time of running model in reality\n| Time related | scheduled_departure_UTC_add_2hr | timestamp | timestamp used to match destination weather at time of running model in reality \n| Time related | roundedMonth | timestamp | month and year of flight \n  \n  \n**Number of Variables in Feature Families**\n| Feature Family | Number of Variables\n|----------------|--------------------|\n| Classification label | 1\n| Regression target variable | 1\n| Time related | 8\n| Flight Information | 5\n| Plane History | 5\n| Origin Information | 6\n| Destination Information | 4 \n| Holiday | 2\n| Covid-19 | 1\n| Origin Weather | 22 \n| Destination Weather | 22\n| Other | 12\n| 10 Families | 89 Features"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"91aaba4b-f8fe-4869-a0bb-2f7e17b8cf6d","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# EDA\n\nWe continued with EDA for Phase IV to further evaluate our feature engineered variables and the data. Following from Phase III EDA, we've determined that our C19 feature engineered variable will account for the significant changes in delays and delay times. These changes were not seasonal or structural changes but due to black swan events due to government coordination around the world. Looking at our 'holiday' and 'two day holiday range' feature engineered variables, based on prior EDA we determined this would be a helpful variable to engineer however with additional due diligience of EDA against our labels, it appears as though the holidays that would be most impacted with delays is Thanksgiving holidays and potentially Christmas. Other US holidays are less affected by delays and do not have any more delays than the rest of the year. This variable may not be as effective as we hoped but it is a good learning opportunity. Reviewing our labels against other features such as Carrier, we see there are clear trends where some airlines are more likely to be delayed than others. The trend appears to be premium airlines (e.g., Hawaiian) are likely to have delays compared to budget airlines (e.g., Frontier). \n\nComparing our labels against time blocks, we see that there are significant differences between time blocks which would bode well for increasing the accuracy of our models. Furthermore, comparing our label against type we see there are differences between smaller and larger airports where the latter are more likely to have delays and the former are more likely to have cancellations. Including this variable would be beneficial to the accuracy of our model. \n\nWe also reviewed the span of delays in terms of minutes and there is a significantly long tail in the DEP_DELAY_NEW across multiple features that we've observed (e.g., Month, Carrier, Airport Type, Time Block, etc.). So much so that we've noticed the 50th percentile of delay times is ~0 minutes, and 75th percentile is generally less than 15 minutes, dependent on the variable you are reviewing. We've accounted for this in our models to account for sigificant outliers. \n\nTo improve notebook performance, the EDA functions and visuals are available in the following [Phase IV EDA notebook](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/632558266977013/command/1215577238242060).\n\nPrior EDA done on these datasets can be found in the below notebooks:\n- <a href=\"https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/844424046180181/command/917638415058265\" target=\"_blank\">Phase I (Raw Data) Q1<a/>\n- <a href=\"https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1020093804825911/command/1020093804825965\" target=\"_blank\">Phase II (Joined Data) <a/>\n- <a href=\"https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/2647101326221675/command/2647101326221746\" target=\"_blank\">Phase III (Feature Engineered Data) <a/>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eea285e8-e81d-4104-8dd3-4f23735cdc16","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# spark.read.parquet(f\"{blob_url}/df_main_fullJoin\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"222de419-d1ef-40a7-b4b0-47d7442df162","inputWidgets":{},"title":"Joined Dataframe"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# spark.read.parquet(f\"{blob_url}/df_main_fullClean\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"b291da95-2ee9-4059-9497-ee61b3c12324","inputWidgets":{},"title":"Final Cleaned & Feature Engineered Dataframe "}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Modeling"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f31c31c7-146b-4a24-9999-c5719656967a","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Modeling Metrics\n\n#### Classification Metrics\nIn evaluating our classification models, we will consider the following four metrics:\n \n**Precision:** Defined as proportion of correctly predicted positives of total predicted positives, can also be referred to as positive predictive value. \n\n$$ Precision = \\frac{TP}{TP+FP} $$\n\n**Recall:** Defined as proportion of correctly identified positives, can also be referred to as the true positive rate or sensitivity.\n\n$$ Recall = \\frac{TP}{TP+FN} $$\n\n**F1-score:** Measure of a model's accuracy. It is calculated from the precision and recall of the model, where the precision is the number of true positive results divided by the number of all positive results\n\n$$ F1 = \\frac{2 * Precision * Recall}{Precision + Recall} = \\frac{2 * TP}{2 * TP + FP + FN} $$\n\n**Accuracy:** All true prediction over all predictions.\n\n$$ Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}  $$\n  \nOf these four metrics we will give greater emphasis on F1 score and accuracy. F1 score considers both precision and recall. In the case of predicting flight delays, a higher precision refers implies that if we predict a flight will be delayed, we are more relatively confident in the classification the model made. Recall will refer to the ability of catching all flights that could be delayed. So while we do want our model to capture all flights that could be delayed, so that airport staff can at least be aware and plan accordingly, we still believe that it is important for the model to be confident in the classification it is assigning. As the F1 score considered both precision and recall, we will look for a 'relatively good' F1 score, that has relatively high precision and recall. More concretely, this means that when we will be evaluating models with pretty similar F1 scores, the F1 score with relatively higher precision and recall may be considered to be the better model. As there is a tradeoff between precision and recall, being able to find a model with high scores for both will be more difficult to achieve, even if it is the goal. \n\nAccuracy is something we will also consider because we want a model that is overall accurate at predicting whether a flight will be delayed or not. In particular we will hoping to have an accuracy to of at least 80%, because ~80% of the flights in our final dataset are not delayed, implying that there is an 80% chance at random that a flight will not be delayed. We want to work towards developing a more accurate model. \n\n\n#### Regression Metrics \n  \nIn evaluating the regression models, we will consider the below metrics.  \n  \n**Root Mean Squared Error (RMSE):** Defined as the square root of the mean squared error (MSE). RMSE is commonly used because it provides a goodness of fit metric that is in the sample units as the target variable, in this case minutes. \n\n$$ RMSE = \\sqrt{MSE} $$ where\n$$ MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\widehat{y}_i)^2$$\n  \n**Mean Absolute Error (MAE):** Defined as the average of the absolute value of difference between the estimated values and the actual value. MAE is in the same units as the target variable, in this case minutes. Because MSE includes the square difference rather than the absolute difference, MSE will penalize outliers more than MAE.\n\n$$ MAE = \\frac{1}{n}\\sum_{i=1}^{n} \\lvert y_i - \\widehat{y}_i \\rvert $$\n\n**R-Squared (R^2):** R-squared is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model.\n\n$$ R^2 = 1 - \\frac{RSS}{TSS} $$\n\n**Adjusted R-Squared (Adj. R^2):** Adjusted R-squared is similar in nature to R-Squared but it is weighted against the number of independent variable used; SS \n\n$$ R^2 = 1 - \\frac{SS_{res} / (n - K)}{SS_{tot} / (n - 1)}  = 1 - \\frac {(1 - R^2)(N - 1)} {(N - p - 1)}$$\nwhere R^2 is the sample R^2, N is the total sample size, and p is the number of features. \nFor all of our regression models, we will be comparing first the MAE and then RMSE. According to the RMSE formula, the square of the error between the actual and predicted value, implies that larger errors will get penalized more. As MAE considers the absolute values of the error (see formula above), it is not penalizing large errors as heavily as RMSE would, and makes it more robust to outliers, which given the outliers in our dataset, is appropriate in our situation. In our context, MAE is in units of minutes of a delayed flight. \n\nWhen comparing linear regression models to other linear regression models, we could also consider the R^2. Currently the MLlib library does not have the Adjusted R^2 calcuation which would consider the number of features we have in the model and be more appropriate for comparing linear regression models. But as we will be looking at other tree based regression models, we find that it will be better to focus more on MAE. Furthermore, we choose to use MAE rather than RMSE as our model evaluation metrics for regression models because it handles outliers better. In the case of delay times, the outliers were significant which justified MAE's use over RMSE."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"12edf1bf-3100-4be8-875b-537b14d60b2f","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Modeling Pipeline\n\nThe below image shows our modeling pipeline: <br/>\n<img src=\"https://raw.githubusercontent.com/brianahart/spark_flight_predictions/main/Modeling%20Pipeline.jpeg\" width=\"1000px\" height=\"100px\"> \n\nThe below table mentions the different functions used in our modeling pipeline. We will now explain how the below functions are operationalized to follow the modeling pipeline we show in the above image. \n\nFor each model/experiment that we run, we start off by listing our categorical variables that will be one hot encoded and any numeric variables that did not need any null imputations that need to be scaled. We then pass it through the create_pipeline function. Within the create_pipeline function it takes the selected features that do not need any data imputations and creates a vector of categorical and another vector of continuous features. We then take the dataframe and split it into the training and validation sets with the grid_search_test_train_split function. In this function, the data gets split to train and validation (test as well if testing after experimentation), applies any sampling (over or under) if specified, and then imputes any nulls and adds in the time based features (aided by the impute_and_scale_features and pre_Modeling_dataEdit functions). Finally, we run the train_model_no_CV uses the get_param_permutations function to get all combinations of the grid search that we need to experiment with, build the models with get_model, and then evaluate the model with evaluate_model. This is the process for running the experiments without cross validation to perform the grid search. To do cross validation on a chosen model with specific parameters, we run the create_pipeline function again, and then immediately run the train_model_CV() function which takes care of splitting the dataset, imputing the nulls and time based features on each train and test subset in each fold (number of folds is an argument), running the model, and getting evaluation metrics. \n\n**Functions Used in Modeling Pipeline** \n| Function | Input | Output |\n|----------------------|-------------------------------|-------------------------------|\n|preModeling_dataEdit()| df that has already gone through the final join, cleaning, and feature engineering | df that includes null imputing and # and % of flights (by tail number) that were delayed and cancelled in the past 90 days --> these depend on window functions, as such they need to be done right after the data is split for modelling and not during feature engineering phase |\n| create_pipeline() | inputCols_cat (list): list of categorical input cols <br/> inputCols_cont (list): list of continuous input cols | pipeline (Pipeline): MLlib pipeline with stages  |\n| impute_and_scale_features() | all features | imputed features, scaled features, and one hot encoded features all in a vector | \n| get_sampling() | train_df (df): training data <br/> sampling (string): if none, no sampling is performed; if under, undersampling is performed; if over, oversampling is performed | train_df_sampled (df): modified training data |\n| get_model() | model_type (string): type of model to be built <br/> params (dict): dictionary of parameters specific to the model_type | model: MLlib model ready to be trained <br/> ml_type (string): type of model (classification or regression) |\n| get_param_permutations() | params (dict): dictionary of parameters inputted by user | param_list (list): list of dictionaries to pass to the model |\n| evaluate_model() | predictions (df): dataframe of predicated and actual values <br/> ml_type (string): type of model | classification: accuracy, precision, recall, f1score <br/> regression: r2, rmse, mse, mae |\n|train_model_no_CV() | train_df (df): training data that has been through <br/> grid_search_test_train_split <br/> val_df (df): validation data that has been through grid_search_test_train_split <br/> model_type (string): indicates the type of model that will be trained <br/> params (dict): a dictionary of parameters as keys and list of parameter values as values | results_df (df):  dataframe of parameters tested and the results from that iteration |\n| grid_search_test_train_split() | df (dataframe): dataframe to model on; requirements: has gone through create_pipeline function, has 'Year' column from 2015 - 2021, has 'features' column (not scaled), has 'label' column <br/> sample_size (float): optional parameter to specify if you would like a subset of the data <br/> sampling (string): if none, no sampling is performed; if under, undersampling is performed; if over, oversampling is performed | results_df (df):  dataframe of parameters tested and the results from that iteration |\n| train_model_CV() | df (dataframe): dataframe to model on <br/> requirements: has gone through create_pipeline function, has 'Year' column from 2015 - 2021, has 'features' column (not scaled), has 'label' column <br/> model_type (string): indicates the type of model that will be trained <br/> params (dict): a dictionary of parameters as keys and parameter values as values <br/> k (int): number of folds to split data into <br/> sampling (string): if none, no sampling is performed; if under, undersampling is performed; if over, oversampling is performed | results_df (df):  dataframe validation results from each fold <br/> saved_model (model): returns the model that had the best validation performance of each fold |"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dcac6801-2303-4ebd-a0c0-d1c51b5166b4","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Data Leakage\n\n*Data leakage* refers to the situation when information outside of the training dataset is being used to develop/train the model. It allows the model in question to learn something it would otherwise not know when trying to predict. This situation can make it look like the model is performing too well as it will inflate its true evaluation metrics. If data leakage occurs, one would have to call into serious question the validity and quality of the machine learning model developed. \n\nFor this reason, we have taken several measures throughout the data and modeling pipeline to ensure that no information is shared between the training data and any validation or testing data. In practical terms, this meant being aware of when different rows/observations were interacting with each other. Below we outline the steps we took to prevent any data leakage. \n\n*Data Pipeline I: Raw Data to Join Execution*\n- Any cleaning done prior to the join was done using information from the record itself\n  - Converting filght date and time to UTC\n  - Filtering (such as for weather stations in the United States) did not change any existing record values.\n  - Every join (preliminary and final joins) was done by matching each record to another; there was no situation in the joins where any existing record's value changed. There were some new columns added to each record (e.g. scheduled departure time in UTC) but they were calculated using only record specific information.\n\n*Data Pipeline II: Further Cleaning and Feature Engineering*\n- Further Cleaning:\n  - Data type conversions did not pose a risk for leakage as it affects each record independently.  \n  - Trimming lead/trail spaces in string columns affects each record individually; no interaction between records. \n  - Extra cleaning for the different types of delays (carrier, weather, etc.) was done using values in the same row.\n  - Splitting up the origin and destination sky condition variable into multiple binary variables only required information from the same observation.\n- Feature Engineering:\n  - Year, dep_delay_15, the classification label, holiday variables, and the Covid-19 variable were all calculated based on values in the same row/observation; none of the rows interacted with each other. \n\n*Modeling Pipeline*\n- In the modeling pipeline, the only steps done before splitting the data in to training, validation, and testing groups is assembling the features into vectors which does not have any rows interacting with each other. Only after the data is split into training, validation, and testing sets, do we run a function that imputes nulls and creates new time-based features that are calculated based on specific rows (location and hour in our case). In the case of cross validation, we run the function to impute nulls and create new time based features after the folds and the train/test sets for each fold are created; i.e. we apply our function for each individual train and test set in each fold. \n\nBy moving any calculations that need to be done using any other rows after the data is split, we were able to prevent any data leakage in our model development."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f2afb1a6-3fc0-4c33-ae1a-219e934003e4","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Baseline Models\n\nBefore conducting any experiments, we developed classification and regression baseline models ([here](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1860389250617691/command/1215577238238661)) to have some level of comparison when conducting the experiments. These baseline models were trained on data from 2015-2019 and validated on 2020 data. We decided to hold out our 2021 data for after the experimentation phase to get a more realistic idea of how our final models do when they see unseen data.  \n\n**Classification** <br/>\nWe decided to use a simple logistic regression model as a basis of comparision for our classification models. It does not have any sampling or regularization parameters added to it, and it goes through maximum 10 iterations of the data. Looking at the scores below it does look like we have relatively strong scores to start off with. The precision and recall on the validation set is relatively high (0.9147 and 0.9838 respectively). In particular the accuracy and precision for the validation set seems to be somewhat significantly higher than training - perhaps indicating the validtaion data was less complex and easier to predict correctly with higher confidence. It is important to note however that in our full dataset, only 18% are labeled as delayed and making our dataset [moderately imbalanced](https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data). So the high level metrics could be somewhat more inflated than what they should be. But this is something we will be able to confirm more throughout our experimentation. Moreover, as there is an 82% chance of a flight not being delayed at random (given our dataset imbalance), we would like to develop a model that is better than randomly guessing and has a higher accuracy than 82%.\n\n**Regression**<br/>\nThe regression baseline model is a linear regression model with no regularization parameters and 10 iterations. As we mentioned in our model metrics section (master notebook), we will mainly be focusing onE, with a secondary focus on RMSE. The MAE in the baseline model is 15.4381, which translates to an error of this baseline model predicting a delay time +/- 15 minutes. It is also interesting to see the RMSE is not too much larger than the MAE, which is a good sign. It indicates that the model is able to somewhat handle outliers. As we moved further down the experiments, we realized there were some results were the RMSE was much much larger than the MAE. We hope that in our final model we can achieve an MAE lower than this so as to predict with higher confidence the number of minutes a flight will be delayed."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"86888aae-2c1d-40ce-920f-b9bbaa7fd3b0","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Experimentation Setup\n\nContinuing from Phase 3 where we focused on building out simplier models, in Phase 4 we started implementing more complex ML models such as Random Forests, Gradient Boosted Trees, MLP neural network. In order to determine the best hyperparameters we conducted various experiments, noting the results of each and fine tuning the model for increased accuracy. The increased accuracy does come at at a risk of potentially overfitting the model (vs underfitting). This is also known as the bias-variance trade-off where we fine tune the model to the find the appropriate balance between bias and variance. Bias can represent the difference between the average prediction and the true value and variance measures how much on average predictions can vary for a given data point. We can find the appripriate balance by trading some bias for variance (i.e. increasing the complexity of our model) and thus find a happy medium between underfitting and overfitting. For each model, the hyperparameters that were tuned and model scoring are explored further below.\n\nWe first ran experiments without cross validation and then with cross validation (further information in following sections). \n\n### Classification \n#### Logistic Regression\n##### Hyperparameters:\n - Max Iterations: The maximum number of iterations that the model will run on the training data. \n - Regularization Parameter: The \\\\(\\lambda\\\\) term in the equation below. Regularization parameter and elastic net parameter together control L1 and L2 regularization for logistic regression models. \n - Elastic Net Parameter: The \\\\(\\alpha\\\\) term in the equation below. \n $$\\alpha (\\lambda ||w||_1) + (1-\\lambda)\\left( \\frac{\\lambda}{2} ||w||_2^2 \\right) $$\n\n##### Loss Function (with Regularization):\n\n$$Log Loss = \\frac{1}{n} \\sum_{i=1}^{n} -y log(y{}') - (1-y)log(1-y{}')+\\alpha (\\lambda ||w||_1) + (1-\\lambda)\\left( \\frac{\\lambda}{2} ||w||_2^2 \\right)$$\n\n#### Decision Tree Classification \n##### Hyperparameters:\n - Max Depth: The maximum depth of the tree. \n - Impurity: The criterion used for information gain calculation. \n - Max Bins: Number of bins used when discretizing continuous features.\n\n##### Loss Function:\n$$Gini = 1-\\sum_{i=1}^n (p_i)^2 $$\nwhere p is the probability of entropy\n\n$$Entropy = \\sum_{i=1}^{c}-p_ilog_2p_i$$\nwhere \\\\(p_i\\\\) is the probability of an object being in a certain class\n\n\n#### Random Forest Classification \n##### Hyperparameters:\n - Max Depth: The maximum depth of each tree.\n - Number of Trees: Number of trees to make up the forest. \n - Impurity: The criterion used for information gain calculation. \n - Max Bins: Number of bins used when discretizing continuous features.\n - Minimum Info Gain: Minimum amount of information a feature provides about a class, helps determine the order of attributes and the nodes of the tree.\n\n##### Loss Function:\n\n$$Gini = 1-\\sum_{i=1}^n (p_i)^2$$\nwhere p is the probability of entropy\n\n$$Entropy = \\sum_{i=1}^{c}-p_ilog_2p_i$$\nWhere \\\\(p_i\\\\) is the probability of an object being in a certain class\n\n\n#### Multilayer Perceptron Classifier\n##### Hyperparameters:\n - Max Iterations: The maximum number of iterations that the model will run on the training data. \n - Step Size: Model learning rate. \n - Block Size: Number of inputs to be included during each iteration.\n - Tol: Refers to convergence tolerance for which the model's weights converge to the predicted answer\n - Seed: Random Seed\n - Solver: Solver algorithm for optimization\n\n##### Loss Function (with Regularization):\n\n$$MSE = \\frac{1}{n} \\sum_{i=1}^{n}(Y_i - \\widehat{Y_i})^2+\\alpha (\\lambda ||w||_1) + (1-\\lambda)\\left( \\frac{\\lambda}{2} ||w||_2^2 \\right)$$\n\n\n### Regression \n#### Linear Regression\n##### Hyperparameters:\n - Max Iterations: The maximum number of iterations that the model will run on the training data. \n - Regularization Parameter: The \\\\(\\lambda\\\\) term in the equation below. Regularization parameter and elastic net parameter together control L1 and L2 regularization for linear regression models. \n - Elastic Net Parameter: The \\\\(\\alpha\\\\) term in the equation below. \n $$\\alpha (\\lambda ||w||_1) + (1-\\lambda)\\left( \\frac{\\lambda}{2} ||w||_2^2 \\right)$$\n\n##### Loss Function (with Regularization):\n\n$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}|y_{i} - \\widehat{y}_{i}|+\\alpha (\\lambda ||w||_1) + (1-\\lambda)\\left( \\frac{\\lambda}{2} ||w||_2^2 \\right)$$\n\n\n#### Decision Tree Regression\n##### Hyperparameters:\n - Max Depth: The maximum depth of the tree. \n - Minimum Info Gain: Minimum amount of information a feature provides about a class, helps determine the order of attributes and the nodes of the tree. \n\n##### Loss Function:\n\n$$Gini = 1-\\sum_{i=1}^n (p_i)^2$$\nwhere p is the probability of entropy\n\n$$Entropy = \\sum_{i=1}^{c}-p_ilog_2p_i$$\nWhere \\\\(p_i\\\\) is the probability of an object being in a certain class\n\n#### Random Forest Regression\n\n##### Hyperparameters:\n - Max Depth: The maximum depth of each tree.\n - Number of Trees: Number of trees to make up the forest. \n - Minimum Info Gain: Minimum amount of information a feature provides about a class, helps determine the order of attributes and the nodes of the tree.\n\n##### Loss Function:\n\n$$Gini = 1-\\sum_{i=1}^n (p_i)^2$$\nwhere p is the probability of entropy\n\n$$Entropy = \\sum_{i=1}^{c}-p_ilog_2p_i$$\nWhere \\\\(p_i\\\\) is the probability of an object being in a certain class\n\n\n#### Gradient Boosting Trees Regression\n##### Hyperparameters:\n - Max Depth: The maximum depth of each tree.\n - Max Iterations: The maximum number of iterations that the model will run on the training data. \n - Step Size: Model learning rate. \n - Minimum Info Gain: Minimum amount of information a feature provides about a class, helps determine the order of attributes and the nodes of the tree.\n\n##### Loss Function (with Regularization):\n\n$$MSE = \\frac{1}{n} \\sum_{i=1}^{n}(Y_i - \\widehat{Y_i})^2+\\alpha (\\lambda ||w||_1) + (1-\\lambda)\\left( \\frac{\\lambda}{2} ||w||_2^2 \\right)$$\n\n\n### Input Features \n\nWe included all of the below features in our models, with the exception of two follow up experiments in logistic and linear regression in which we excluded elevation ft, origin and destination wet bulb temperature, origin and destination wet dew temperature, origin and destination pressue change, and origin and destination wind gust speed due to high correlation with other existing featuress. More detail on the justification of excluding these features can be found in the full experiments section in this notebook. It should also be known that in prior iterations of our experiments we included a few extra features such as both count and time of flights delayed in the past 24 hours. We realized that those two variables bring in similar information and could further confuse our models; as such we decided to stick with percentage of flights delayed (or cancelled). \n\n|Total number of features by family | Count |\n|-------------------------|------|\n| Time related | 6 |\n| Flight information | 4 |\n| Aircraft History | 2 |\n| Origin information | 3 |\n| Destination Information | 1 |\n| Holiday | 1 |\n| Covid 19 | 1 |\n| Origin Weather | 22 |\n| Destination Weather | 22 | \n\n|Feature Family | Variable | Data Type | Definition |\n|---------------|----------|-----------|------------|\n| Classification label | \\*label | integer | classification label; 0 = on time/early/delayed by less than 15 minutes, 1 = delayed by 15+ minutes\n| Regression Target Variable | \\*DEP_DELAY_NEW | integer | regression label; number of minutes flight is delayed \n| Time related | \\*Year | integer | year of flight date \n| Time related | \\*QUARTER | integer | quarter of flight date\n| Time related | \\*MONTH | integer | month of flight date\n| Time related | \\*DAY_OF_MONTH | integer | day of month of flight date\n| Time related | \\*DAY_OF_WEEK | integer | day of week of flight date\n| Time related | \\*DEP_TIME_BLK | string |scheduled depature time (hourly blocks)\n| Flight information | \\*OP_UNIQUE_CARRIER | string | flight airline; identifier assigned by US Department of Transportation for airlines\n| Flight information | \\*TAIL_NUM | string | identifier for specific aircraft\n| Flight information | \\*OP_CARRIER_FL_NUM | string | flight number\n| Flight information | \\*DISTANCE | integer | planned flight distance\n| Plane History | \\*perc_delays_last1d | double | percentage of flights delayed in past 24 hours per tail number\n| Plane History | \\*perc_cancellation_last1d | double | percentage of flights cancelled in past 24 hours per tail number\n| Origin Information | \\*ORIGIN_AIRPORT_ID | string | origin airport ID; assigned by US Department of Transportation \n| Origin Information | \\*elevation_ft | integer | origin airport elevation\n| Origin Information | \\*type | string | airport size (small, medium, large)\n| Destination Information | \\*DEST_AIRPORT_ID | string | destination airport ID; assigned by US Department of Transportation \n| Holiday | \\*holiday_in2DayRange | long | whether flight date is a federal holiday +/- 2 days\n| Covid-19 | \\*C19 | integer | ordinal variable referring to impact COVID-19 was having on aviation; further information <a href=\"https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1325974983871287/command/1325974983871304\" target=\"_blank\">here<a/>\n| Origin Weather | \\*origin_HourlyAltimeterSetting | float | hourly pressure (hectopascals) value which altimeter is set to indicate altitude relative to mean sea level of aircraft on the ground at location for which value was determined; at origin\n| Origin Weather | \\*origin_HourlyDewPointTemperature | integer | hourly temperature (Celsius) which a given parcel of air must be cooled at constant pressure and water vapor content in order for saturation to occur; at origin\n| Origin Weather | \\*origin_HourlyDryBulbTemperature | integer | hourly temperature (Celsius) and is another standard way to measure air temperature; at origin\n| Origin Weather | \\*origin_HourlyPrecipitation | float | hourly rain quantity measured in millimeters; at orgin \n| Origin Weather | \\*origin_HourlyPressureChange | float | hourl number indicating air pressure change; measured in hectopascals; at origin\n| Origin Weather | \\*origin_HourlyPressureTendency | integer | hourly air pressure measured in hectopascals; at origin\n| Origin Weather | \\*origin_HourlyRelativeHumidity | integer | the amount of water vapour present in air expressed as a percentage of the amount needed for saturation at the same temperature; at origin\n| Origin Weather | \\*origin_HourlySkyConditions | string | hourly combination of codes which denote tany specific conditions in sky (e.g. SCT:04 45 SCT:04 190); at origin\n| Origin Weather | \\*origin_HourlySkyConditions_SCT_cnt | integer | number of scattered cloud layers; at origin \n| Origin Weather | \\*origin_HourlySkyConditions_OVC_cnt | integer | number of overcast cloud layers; at origin  \n| Origin Weather | \\*origin_HourlySkyConditions_FEW_cnt | integer | number of 'few' cloud layers; at origin \n| Origin Weather | \\*origin_HourlySkyConditions_BKN_cnt | integer | number of broken cloud layers; at origin \n| Origin Weather | \\*origin_HourlySkyConditions_VV_cnt | integer | number of vertical visibility cloud layers; at origin \n| Origin Weather | \\*origin_HourlySkyConditions_SKC_cnt | integer | number of clear cloud layers; at origin  \n| Origin Weather | \\*origin_HourlySkyConditions_CLR_cnt | integer | number of clear cloud layers; at origin \n| Origin Weather | \\*origin_HourlySeaLevelPressure | float | hourly air pressure (hectopascals) relative to mean sea level; at origin\n| Origin Weather | \\*origin_HourlyStationPressure | float | hourly atmospheric pressure at a weather station (hectopascals); at origin \n| Origin Weather | \\*origin_HourlyVisibility | float | hourly horizontal distance at which an object can be seen/identified; at origin \n| Origin Weather | \\*origin_HourlyWetBulbTemperature | integer | hourly average wet bulb temperature (Celsius); at origin \n| Origin Weather | \\*origin_HourlyWindDirection | integer | hourly angle measured in blockwise direction between true north and direction from which the wind is blowing; at origin \n| Origin Weather | \\*origin_HourlyWindGustSpeed | integer | hourly rate of speed of wind gust measured in meters/second; at origin \n| Origin Weather | \\*origin_HourlyWindSpeed | integer | hourly rate of speed of wind, measured in meters/secondl at origin \n| Origin Weather | \\*dest_HourlyAltimeterSetting | float | hourly pressure value (hectopascals) which altimeter is set to indicate altitude relative to mean sea level of aircraft on the ground at location for which value was determined; at destination\n| Destination Weather | \\*dest_HourlyDewPointTemperature | integer | hourly temperature (Celsius) which a given parcel of air must be cooled at constant pressure and water vapor content in order for saturation to occur; at destination\n| Destination Weather | \\*dest_HourlyDryBulbTemperature | integer | hourly temperature (Celsius) and is another standard way to measure air temperature; at destination \n| Destination Weather | \\*dest_HourlyPrecipitation | float | hourly rain quantity measured in millimeters; at destination \n| Destination Weather | \\*dest_HourlyPressureChange | float | hourly number indicating air pressure change; at destination \n| Destination Weather | \\*dest_HourlyPressureTendency | integer | hourly air pressure measured in hectopascals; at destination\n| Destination Weather | \\*dest_HourlyRelativeHumidity | integer | the amount of water vapour present in air expressed as a percentage of the amount needed for saturation at the same temperature; at destination\n| Destination Weather | \\*dest_HourlySkyConditions | string | hourly combination of codes which denote tany specific conditions in sky (e.g. SCT:04 45 SCT:04 190); at destination\n| Destination Weather | \\*dest_HourlySkyConditions_SCT_cnt | integer | number of scattered cloud layers; at destination \n| Destination Weather | \\*dest_HourlySkyConditions_OVC_cnt | integer | number of overcaset cloud layers; at destination \n| Destination Weather | \\*dest_HourlySkyConditions_FEW_cnt | integer | number of 'few' cloud layers; at destination \n| Destination Weather | \\*dest_HourlySkyConditions_BKN_cnt | integer | number of broken cloud layers; at destination \n| Destination Weather | \\*dest_HourlySkyConditions_VV_cnt | integer | number of vertical visibility cloud layers; at destination \n| Destination Weather | \\*dest_HourlySkyConditions_SKC_cnt | integer | number of clear cloud layers; at destination \n| Destination Weather | \\*dest_HourlySkyConditions_CLR_cnt | integer | number of clear cloud layers; at destination \n| Destination Weather | \\*dest_HourlySeaLevelPressure | float | hourly air pressure (hectopascals) relative to mean sea level; at destination\n| Destination Weather | \\*dest_HourlyStationPressure | float | hourly atmospheric pressure at a weather station (hectopascals); at destination\n| Destination Weather | \\*dest_HourlyVisibility | float | hourly horizontal distance at which an object can be seen/identified; at destination \n| Destination Weather | \\*dest_HourlyWetBulbTemperature | integer | hourly average wet bulb temperature (Celsius); at destination \n| Destination Weather | \\*dest_HourlyWindDirection | integer | hourly angle measured in blockwise direction between true north and direction from which the wind is blowing; at destination \n| Destination Weather | \\*dest_HourlyWindGustSpeed | integer | hourly rate of speed of wind gust measured in meters/second; at destination \n| Destination Weather | \\*dest_HourlyWindSpeed | integer | hourly rate of speed of wind, measured in meters/secondl at destination"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c05bd000-bcfd-41d5-9567-827e3314efb0","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Experiments Without Cross Validation\n\nWe ran experiments on 4 different classification models (logistic regression, decision tree, random forest, multiple layer perceptron neural network), and 4 different regression models (linear regression, decision tree, random forest, gradient boosted trees). For the classification models we experimented with no, over, and under sampling. [This](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1860389250617691/command/1215577238238662) notebook contains the experiments on which hyperparameters will work best, without cross validation on all the models except the neural network. The neural network experiments can be found [here](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1860389250614781/command/1860389250614814). The experiments done in this notebook are meant to inform which models we would like to run using cross validation. As running cross validation on a dataset as large as ours can be very time costly, we decided to be selective with our models. \n\nOur plan was to first test on smaller samples and then scale up; specifically from 0.1% sample size, to 10%, and finally the full dataset. The idea was to conduct a larger grid search on 0.1% sample, then a smaller one on 10%, and then finally a much smaller grid search on the full dataset. This way we could iterate more efficiently and effectively. But we found during our experimentation process that as we tried to scale up, our models that were doing the best in 0.1% would degrade very quickly at 10%. So by the time we reached full data experimentation, we decided to run a grid search that was slightly smaller than what we ran at 0.1% and slightly larger than at 10%, but not necessarily always based off of results from 0.1%. \n\n*Challenges* <br/>\nIt should also be noted that we ran into a few issues in the experimentation phase. For one, we realized that our time-based feature engineered variables on the number and percentage of flights delayed and cancelled (4 variables) may have contained some stale data. In that after some reflection, we realized that it might be more important to consider if flights were delayed recently because those consequential effects may have more effect on the current flight prediction than something that happened up to 3 months ago. As such we changed our time based variables to reflect the past 24 hours only, and reran the experimentation again for 0.1% and 10%.\n\nAt this point in time we had been running multi-class classification to predict whether a flight would be delayed, cancelled, or not. In the experimentation phase, we also realized that the our models were still performing very poorly and we realized that even with over sampling, it was getting difficult for our models to differentiate between cancellation and delays (see below screen shot where way more flights are being predicted as cancelled than they should be). Especially considering that we had only ~2% of our full dataset with cancellation labels. Even with over and under sampling, our results weren't improving as much, and we knew that having to apply oversampling with our full dataset would be a logistical challenge given the time crunch. We also realized that depending on what rows were being sampled, the validation set may or may not have had rows with cancelled flights. Given this situation, and the fact that we didn't feel we had enough information in our dataset to differentiate between a moderately rare (delay) and very rare (cancelation) events, we decided to go focus on predicting flight delays only (i.e. binary classification) and investigate in future studies on predicting between delays and cancellations.\n\nFor the Multi-Layer Perceptron Classifier (MLPC) model experimentation, there was a learning curve to understanding what needed to be done to get the models to fit the training data. The first challenge was being able to get the exact input layer number, the length of our feature vector. This didn't seem challenging at first; however, we kept getting an error relating to the dimensions of the model. We ended up learning the method we used to find the number of features was incorrect. The first layer should reflect the number of input features which was not the same as the number of raw columns before encoding. Because this number could change we needed to access this through the metadata using the schema of the model transformed. Additionally, it should be noted that due to constrained computing resources in the final days of the project it was not feasible to run both MLP structures on the full dataset, but we did run cross validation on the first one in the next section. Additionally it should be known that with the [MLlib library](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.MultilayerPerceptronClassifier.html#pyspark.ml.classification.MultilayerPerceptronClassifier.tol), each layer has sigmoid activation function and output layer has softmax.\n\n*Selecting Hyperparameters to Test* <br/>\nWhen choosing the hyperparameters for the linear and tree based models, we consulted the PySpark documentation and prior experience to inform our decisions on which values to test. \n\nFor the linear models we knew that we wanted to experiment with how much regularization we wanted to incorporate and what kind. As such we incorporated both the regularization and elastic new hyper parameters to experiment with the level of and type of regularization (L1, L2; more information in experimentation set up above). \n\nFor all of the tree-based models we wanted to experiment with the tree size (max_depth) to play with the complexity of the trees, and also the minimum information gain required to create a split to battle overfitting because decisions that barely help split the training data may not be great at dividing test data. For the classification models we also experimented with impurity, and the maximum number of bins used to discretize continuous features to help understand what type of tree structure might work best for our data; and for random forests and gradient boosted trees we also experimented with the number of trees (iterations for gradient boosted trees) as well. Finally with gradient boosted trees we also experimented with the step size (i.e. learning rate) to experiment with how fast we can get to convergence. \n\nFor MLPC hyperparameters, we first had to find values which would provide us accurate results without overfitting. Over all of our experiments the model that performed best was determined over the validation's F1 score and so this is what led to the following hyperparameter selection. For example, the hyperparameter tol refers to convergence tolerance for which the model's weights converge to the predicted answer. The default value 0.000001, and choosing a smaller value will lead to overfitting. Over our experiments we saw this to hold true and chose 0.00001, one decimal place above the default tol, to be a good compromise. Block size was increased from the default value of 128 to 300 and step size was left at the default of 0.03. \n\n<img src=\"https://raw.githubusercontent.com/brianahart/spark_flight_predictions/main/Screenshot%202022-12-02%20at%208.02.36%20AM.png\" width=\"600px\"> <br/>\n\n<img src=\"https://raw.githubusercontent.com/brianahart/spark_flight_predictions/main/Screenshot%202022-12-02%20at%208.03.02%20AM.png\" width=\"600px\">\n\n\n*Multi-Layer Perceptron Structures*\n|Structure |\n|---------------------------------------|\n| 5 - Sigmoid - 4 Sigmoid - 2 Softmax |\n| 5 - Sigmoid - 4 Sigmoid - 2 Softmax |\n\n\nThe experimental setup and results in this notebook reflect the updated time based features and binary classification. This served as a basis on what would be further tested in cross validation in our journey to selecting our best performing models. \n\n#### Number of Experiments Conducted (add in MLP)\n\n| Sample | Number of Experiments |\n|--------|------|\n| 0.1%   | 489 |\n| 10%    | 168 |\n| Full   | 43 |\n| TOTAL  | 700 |\n\n#### Sample Size = 0.1%\n| Model Type     | Model                    | Evaluation Metric | Training Time (None/Under/Over Sampling) | Number of Experiments | Sampling        | Hyperparameters | \n| -------------- | ------------------------ | ----------------- | ---------------------------------------- | --------------------- | --------------- | --------------- |\n| Classification | Logistic Regression |  F1 Score, Precision, Recall, Accuracy | 6.40/7.27/7.37 min | 36 | None/Over/Under | Max Iterations: 10, 20 <br/> Regularization Param: 0.2, 0.4 <br/> Elastic Net Param: 0.0, 0.3, 0.8 | \n| Classification | Decision Tree Classification |  F1 Score, Precision, Recall, Accuracy | 7.01/6.50/1.67 min | 84 | None/Over/Under| Max Depth: 5, 10 <br/> Impurity: Gini, Entropy <br/> Max Bins: 28,32,40 <br/> Min Information Gain: 0.0, 0.05 |\n| Classification | Random Forest Classification | F1 Score, Precision, Recall, Accuracy | 14.92/14.93/14.02 min | 216 | None/Over/Under | Number of Trees: 10, 20, 50 <br/> Max Depth: 3, 5, 7 <br/> Impurity: Gini, Entropy <br/> Max Bins: 32, 40 <br/> Min Information Gain: 0.0, 0.05 |\n| Classification | Multiple Layer Perceptron Structure 1 | F1 Score, Precision, Recall, Accuracy | 14.10/7.93/9.22 min | 81 | None/Over/Under | Max Iterations: 10 <br/> StepSize 0.005, 0.03, 0.09 <br/> BlockSize: 50, 128, 300 <br/> Tolerance: 0.0000001, 0.000001, 0.00001 <br/> Solver: l-bfgs, gd|\n| Classification | Multiple Layer Perceptron Structure 2| F1 Score, Precision, Recall, Accuracy | 14.10/7.93/9.22 min | 81 | None/Over/Under | Max Iterations: 10 <br/> StepSize 0.005, 0.03, 0.09 <br/> BlockSize: 50, 128, 300 <br/> Tolerance: 0.0000001, 0.000001, 0.00001 <br/> Solver: l-bfgs, gd|\n| Regression | Linear Regression | MAE, RMSE | 1.06 min | 12 | None | Max Iterations: 10, 20 <br/> Regularization Param: 0.2, 0.4 <br/> Elastic Net Param: 0, 0.3, 0.8 |\n| Regression | Decision Tree Regression | MAE, RMSE | 1.42 min | 6 | None | Max Depth: 5, 10, 15 <br/> Min Information Gain: 0.0, 0.05 |\n| Regression | Random Forest Regression | MAE, RMSE | 14.52 min | 18 | None | Number of Trees: 10, 20, 50 <br/> Max Depth: 3, 5, 7 <br/> Min Information Gain: 0.0, 0.05 |\n| Regression | Gradient Boosted Trees Regression | MAE, RMSE | 1.95 min | 36 | None | Max Depth: 3, 5, 7 <br/> Max Iterations: 15, 50 <br/> Step Size (Learning Rate): 0.1, 0.3, 0.7 <br/> Min Information Gain: 0.0, 0.05 |\n\n\n#### Sample Size = 10%\n| Model Type     | Model                    | Evaluation Metric | Training Time | Number of Experiments | Sampling        | Hyperparameters | \n| -------------- | ------------------------ | ----------------- | ------------- | --------------------- | --------------- | --------------- |\n| Classification | Logistic Regression |  F1 Score, Precision, Recall, Accuracy | 5.65/8.65 min | 10 | Over/Under | Max Iterations: 10, 20 <br/> Regularization Param: 0.2, 0.4 <br/> Elastic Net Param: 0.0, 0.3, 0.8 | \n| Classification | Decision Tree Classification |  F1 Score, Precision, Recall, Accuracy | 15.95 min/4.33 min/38.56 sec | 18 | Over/Under/Under followup| Max Depth: 5, 10 <br/> Impurity: Gini, Entropy <br/> Max Bins: 28,32,40 <br/> Min Information Gain: 0.0, 0.05, 0.07 |\n| Classification | Random Forest Classification | F1 Score, Precision, Recall, Accuracy | 7.82/1.64 min | 20 | Under/Under followup | Number of Trees: 15, 50, 75 <br/> Max Depth: 3, 5 <br/> Impurity: Gini, Entropy <br/> Max Bins: 32 <br/> Min Information Gain: 0.0, 0.05, 0.07 |\n| Classification | Multiple Layer Perceptron Structure 1| F1 Score, Precision, Recall, Accuracy | 49.13 min | 81 | None | Max Iterations: 20 <br/> Step Size 0.005, 0.03, 0.09 <br/> Blocksize: 50, 128, 300 <br/> Tolerance: 0.0000001, 0.000001, 0.00001 <br/> Solver: l-bfgs, gd |\n| Classification | Multiple Layer Perceptron Structure 2| F1 Score, Precision, Recall, Accuracy |  min | 81 | None | Max Iterations: 5 <br/> Step Size 0.005, 0.03, 0.09 <br/> Blocksize: 50, 128, 300 <br/> Tolerance: 0.0000001, 0.000001, 0.00001 <br/> Solver: l-bfgs, gd |\n| Regression | Linear Regression | MAE, RMSE | 7.49 min | 8 | None | Max Iterations: 10, 20 <br/> Regularization Param: 0.2, 0.4 <br/> Elastic Net Param: 0.0, 0.8 |\n| Regression | Decision Tree Regression | MAE, RMSE | 8.11 min | 7 | None | Max Depth: 5, 10, 15 <br/> Min Information Gain: 0.0, 0.05, 0.07 |\n| Regression | Random Forest Regression | MAE, RMSE | 30.22 min | 8 | None | Number of Trees: 15, 50 <br/> Max Depth: 3, 5 <br/> Min Information Gain: 0.0, 0.05 |\n| Regression | Gradient Boosted Trees Regression | MAE, RMSE | 1.30 min | 16 | None | Max Depth: 3, 5 <br/> Max Iterations: 15, 30 <br/> Step Size (Learning Rate): 0.1, 0.3 <br/> Min Information Gain: 0.0, 0.05 |\n\n#### Full Data Set\n| Model Type     | Model                    | Evaluation Metric | Training Time <br/> (None/Over/Under Sampling) | Number of Experiments | Sampling | Hyperparameters | \n|----------------|--------------------------|-------------------|-------------------|---------------------| --------------- | ------------------------ |\n| Classification | Logistic Regression |  F1 Score, Precision, Recall, Accuracy | 1.12 hours/13.76 mins | 8 | None/Under | Max Iterations: 10, 20 <br/> Regularization Param: 0.2, 0.4 <br/> Elastic Net Param: 0.0, 0.8 | \n| Classification | Decision Tree Classification | F1 Score, Precision, Recall, Accuracy | 20.58 min/9.37 min | 4 | None/Under | Max Depth: 5 <br/> Impurity: Gini <br/> Max Bins: 32 <br/> Min. Information Gain: 0.0, 0.01 |\n| Classification | Random Forest Classification | F1 Score, Precision, Recall, Accuracy | 42.68/27.44 mins | 16 | None/Under | Number of Trees: 15, 50 <br/> Max Depth: 3, 5 <br/> Impurity: Gini, Entropy <br/> Max Bins: 32 <br/> Min. Information Gain: 0.01 |\n| Regression | Linear Regression |  MAE, RMSE | 16.94 min | 8 | None | Max Iterations: 10, 20 <br/> Regularization Param: 0.2, 0.4 <br/> Elastic Net Param: 0, 0.8 |\n| Regression | Decision Tree Regression  | MAE, RMSE |  48.21 mins/9.57 min (follow up) | 3 | None | Max Depth: 5, 10 <br/> Min. Information Gain: 0.0, 0.01 |\n| Regression | Random Forest Regression  | MAE, RMSE |  1.34 hours | 2 | None | Number of Trees: 50 <br/> Max Depth: 5, <br/> Min. Information Gain: 0.0, 0.01|\n| Regression | Gradient Boosted Trees Regression | MAE, RMSE | 1.50 mins | 2 | None | Max Depth: 3 <br/> Max Iterations: 15 <br/> Step Size (Learning Rate): 0.1 <br/> Min. Information Gain: 0.0, 0.01 |\n\n#### Best Results Per Model Per Sample Size\n| Sample Size    | Model                    | Best Parameters  | Performance on Validation Set | \n| -------------- | ------------------------ | ---------------- | ------------------ |\n| 1%   | Logistic Regression | Sampling: None <br/> Max Iterations: 20 <br/> Regularization Param: 0.2 <br/> Elastic Net Param: 0.0 | F1 Score: 0.9066 <br/> Precision: 0.9307 <br/> Recall: 0.9979 <br/> Accuracy: 0.9066 |\n| 1%   | Decision Tree Classification | Sampling: Under <br/> Max Depth: 10 <br/> Impurity: Entropy <br/> Max Bins: 40  <br/> Min Information Gain: 0.05| F1 Score: 0.9024 <br/> Precision: 0.9914 <br/> Recall: 0.93302 <br/> Accuracy: 0.9024 |\n| 1%   | Random Forest Classification | Sampling: Over <br/> Number of Trees: 50 <br/> Max Depth: 5 <br/> Impurity: Gini <br/> Max Bins: 40 <br/> Min Information Gain: 0.0 | F1 Score: 0.8997 <br/> Precision: 0.9310 <br/> Recall: 0.9901 <br/> Accuracy: 0.8997 |\n| 1%  | Multiple Layer Perceptron Structure 1 | maxIter: 10 <br/> stepSize 0.09 <br/> blockSize: 50 <br/> tol: 0.00001 <br/> layers: [number of features, 5, 4, 2] | F1 Score: 0.990 <br/> Precision: 1 <br/> Recall: 0.989 <br/> Accuracy: 0.99 |\n| 1%  | Multiple Layer Perceptron Structure 2 | maxIter: 10 <br/> stepSize 0.09 <br/> blockSize: 300 <br/> tol: 0.00001 <br/> layers: [number of features, 10, 15, 2]  | F1 Score: 0.989 <br/> Precision: 1 <br/> Recall: 0.987 <br/> Accuracy: 0.989 |\n| 1%   | Linear Regression | Sampling: None <br/> Max Iterations: 20 <br/> Regularization Param: 0.2 <br/> Elastic Net Param: 0.8 | MAE: 13.4802 <br/> RMSE: 24.68 |\n| 1%   | Decision Tree Regression | Sampling: None <br/> Max Depth: 5 <br/> Min Information Gain: 0.05 | MAE: 9.3928 <br/> RMSE: 43.1001 |\n| 1%   | Random Forest Regression | Sampling: None  <br/> Number of Trees: 10 <br/> Max Depth: 3 <br/> Min Information Gain: 0.05 | MAE: 7.2266 <br/> RMSE: 21.0994 |\n| 1%   | Gradient Boosted Trees Regression | Sampling: None <br/> Max Depth: 3 <br/> Max Iterations: 15 <br/> Step Size (Learning Rate): 0.1 <br/> Min Information Gain: 0 | MAE: 9.2447 <br/> RMSE: 42.5545 |\n| 10%  | Logistic Regression | Sampling: Over <br/> Max Iterations: 20 <br/> Regularization Param: 0.4 <br/> Elastic Net Param: 0.8 | F1 Score: 0.7867 <br/> Precision: 0.8539 <br/> Recall: 1 <br/> Accuracy: 0.7867 |\n| 10%  | Decision Tree Classification | Sampling: Over <br/> Max Depth: 10 <br/> Impurity: Gini <br/> Max Bins: 32 <br/> Min Information Gain: 0.05 | F1 Score: 0.7867 <br/> Precision: 0.8539 <br/> Recall: 1 <br/> Accuracy: 0.7867 |\n| 10%  | Random Forest Classification | Sampling: Under <br/> Number of Trees: 15 <br/> Max Depth: 5 <br/> Impurity: Gini <br/> Max Bins: 34 <br/> Min Information Gain: 0.0| F1 Score: 0.6278 <br/> Precision: 0.8906 <br/> Recall: 0.5879 <br/> Accuracy: 0.5278 |\n| 10%  | Multiple Layer Perceptron Structure 1| maxIter: 20 <br/> stepSize 0.03 <br/> blockSize: 300 <br/> tol: 0.00001 solver: gd | F1 Score: 0.882 <br/> Precision: 0.920 <br/> Recall: 0.9768 <br/> Accuracy: 0.882 |\n| 10%  | Linear Regression | Sampling: None <br/> Max Iterations: 10 <br/> Regularization Param: 0.4 <br/> Elastic Net Param: 0.0 | MAE: 15.8405 <br/> RMSE: 34.3119 |\n| 10%  | Decision Tree Regression | Sampling: None <br/> Max Depth: 5 <br/> Min Information Gain: 0.0  | MAE: 23.4333 <br/> RMSE: 37.7920 |\n| 10%  | Random Forest Regression | Sampling: None <br/> Number of Trees: 50 <br/> Max Depth: 3 <br/> Min Information Gain: 0.05 | MAE: 17.0127 <br/> RMSE: 34.4760 |\n| 10%  | Gradient Boosted Trees Regression | Sampling: None <br/> Max Depth: 5 <br/> Max Iterations: 15 <br/> Step Size (Learning Rate): 0.1 <br/> Min Information Gain: 0.0   | MAE: 14.7590 <br/> RMSE: 33.9956 |\n| Full | Logistic Regression | Sampling: None <br/> Max Iterations: 10 <br/> Regularization Param: 0.2 <br/> Elastic Net Param: 0.0 | F1 Score: 0.8661 <br/> Precision: 0.9091 <br/> Recall: 0.9998 <br/> Accuracy: 0.8661 |\n| Full | Decision Tree Classification | Sampling: None <br/> Max Depth: 5 <br/> Impurity: Gini <br/> Max Bins: 32 <br/> Min Information Gain: 0.0 | F1 Score: 0.8656 <br/> Precision: 0.9090 <br/> Recall: 1 <br/> Accuracy: 0.8656 |\n| Full | Random Forest Classification | Sampling: None <br/> Number of Trees: 3 <br/> Max Depth: 15 <br/> Impurity: Gini <br/> Max Bins: 32 <br/> Min Information Gain: 0.01| F1 Score: 0.8656 <br/> Precision: 0.9090 <br/> Recall: 1 <br/> Accuracy: 0.8656 |\n| Full | Linear Regression | Sampling: None <br/> Max Iterations: 20 <br/> Regularization Param: 0.2 <br/> Elastic Net Param: 0.8 | MAE: 15.3747 <br/> RMSE: 35.3285 |\n| Full | Decision Tree Regression | Sampling: None <br/> Max Depth: 5 <br/> Min Information Gain: 0.0    | MAE: 17.4483 <br/> RMSE: 35.9332 |\n| Full | Random Forest Regression | Sampling: None <br/> Number of Trees: 50 <br/> Max Depth: 5 <br/> Min Information Gain: 0.0 | MAE: 17.6723 <br/> RMSE: 35.8424 |\n| Full | Gradient Boosted Trees Regression | Sampling: None <br/> Max Depth: 3 <br/> Max Iterations: 15 <br/> Step Size (Learning Rate): 0.0 | MAE: 17.7356 <br/> RMSE: 35.9672 |"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"adc6b45e-c214-4359-912d-f841844b720a","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Experiments With Cross Validation\n\nOnce we completed the grid search on the full dataset, we implemented our final cross-validation function on the full dataset to further reduce overfitting in [this notebook](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1215577238253368/command/1215577238259164). The CV training function takes in a dataframe, model type, parameters, number of folds (k), and sampling method. The dataframe needs to have gone through the create pipeline function first to have the proper pre-processed feature columns. The model type can be logisitc regression, linear regression, decision tree classifier, decision tree regressor, random forest classifier, random forest regressor, gradient boosted regressor, and multilayer perceptron neural network. The parameters that go into the cross-validation function are the finalized hyperparameters from the full dataframe grid search. The number of folds controls how many blocks the dataframe is broken into. [(Image Source)](https://www.r-bloggers.com/2020/03/time-series-cross-validation-using-crossval/) \n\n<img src=\"https://raw.githubusercontent.com/brianahart/spark_flight_predictions/main/2020-03-27-image1.webp\" alt=\"neil\" width=\"400px\">\n\nThe time series cross-validation works as portrayed in the image above. The CV function ranks all 2015-2020 data into folds, and then those folds are split into a 70-30 train-validation split so that the most recent data is always in the validation dataset. The function will train a model on each fold of the training data and evaluate the performance on the validation data per fold. The function will return the model that performs best across all of the folds. That best model is then used to evaluate the 2021 test data. \n\nIn this phase we only evaluated the best performing models, as determined by the grid search experimentation. We also evaluated the important features and metrics for the best model. As part of experimentation during this phase, we used the feature importance and metrics to update the parameters that lead to the best models. The tree-based models all include a feature importance which helped us define which features are the most important. \n\nAn important note, in the last week of this project, we faced many resource issues. We could rarely get above 4 workers to run our modeing, so as we approached the end of the project we had to limit our modeling. We had run the grid search on the full dataset, but we were unable to train the full dataset with cross validation. Instead of training the full dataset, we trained a subset (10%) and then we evaluted the models on the full 2021 test dataset.  \n\nThe model below summarizes the models trained during the cross validation phase. \n\n| Model                     | Parameters        | Performance on 2021 Test Set |\n| ------------------------- | ----------------  | ---------------------------- |\n| Logistic Regression       | Max Iter: 10 <br/> Reg Param: 0.2 <br/> Elastic Net Param: 0 | Accuracy: 0.7483620503154581 <br/> Precision: 0.8265878354685714 <br/> Recall: 0.9999426879240375 <br/> F1 Score: 0.7483651464546285 <br/> AUC: 0.7294006115142018 |\n| Decision Tree Classifier  | Max Depth: 5 <br/> Impurity: gini <br/> Max Bins: 32 <br/> Min Info Gain: 0 | Accuracy:  0.7746962905982705 <br/> Precision:  0.8384174161046766 <br/> Recall:  0.9739264975707075 <br/> F1 Score:  0.7746973769045354 <br/> AUC:  0.5956367577530334 |\n| Random Forest Classifier  | Max Depth: 5 <br/> Num Trees: 50 <br/> Impurity: entropy <br/> Max Bins: 32 <br/> Min Info Gain: 0 | Accuracy: 0.7478834816604819 <br/> Precision: 0.8264236420988693 <br/> Recall: 1.0 <br/> F1 Score: 0.7478834816604819 <br/> AUC: 0.6744435331702908 |\n| Neural Network Classifier | Max Iter: 5 <br/> Step Size: 0.09<br/> Block Size: 300 <br/> Tol: 0.00001 <br/> Seed: 1234 <br/> Solver: gd | Accuracy:  0.7478834816604819 <br/> Precision:  0.8264236420988693 <br/> Recall:  1.0 <br/> F1 Score:  0.7478834816604819 <br/> AUC:  0.6734475489499357 |\n| Linear Regression         | Max Iter: 20 <br/> Reg Param: 0.2 <br/> Elastic Net Param: 0.8 | R2:  0.033635435756791066<br/> RMSE:  45.700064222041895<br/> MSE:  2088.4982171180395<br/> MAE:  18.07766424001644 |"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"56ccba96-aec9-4c97-b547-8f5433a52068","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Novel Directions\n\n#### SMOTE Sampling \n\nThe first novel idea that we tested was SMOTE sampling. We built a function that worked with our existing get_sampling function to oversample. The function runs a k-NN clustering algorithm to synthetically create new samples from the minority class. There is no SMOTE function in PySpark so we had to build our own function. The SMOTE function and our testing can be found on [this notebook](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/632558266976312/command/1215577238239427). We built a function that takes in a vectorized dataframe and runs k-NN to find and create similar new datapoints. The function that we wrote is based off of the one in [this article](https://stackoverflow.com/questions/53936850/oversampling-or-smote-in-pyspark). We chose not to use SMOTE sampling for two reasons. First, we got SMOTE working for a subset of the data, but when we compared the SMOTE sampled results to the non-sampled results, using no sampling performed better. We saw that in general, models without any sampling performed better than models with under or over sampling. Second, and more importantly, SMOTE is very computationally heavy. Our function required the use of a double for loop, which was very computationally heavy. We ran into many resource issues in the final week of the project which made any additional complex functionality difficult to implement. We found that a majority of the time we were only able to get up to 4 workers, which made iteration very difficult. Due to these reasons we decided to stop using the SMOTE sampling in our modeling. \n\n\n#### Streaming Classification Predictions into Regression Models\n\nOur second novel idea that we tested was feeding our predictions from Multi-Layer Perceptron Classifier to see if it has any benefit in a given Regression model. The testing was done so that our predictions dataframe from the MLP model was modified slightly to remove metadata columns and rename the original 'predictions' column. Our hypothesis was to enhance our regression models to decrease error. [In this notebook](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1215577238255625/command/1215577238256195) we have our tests done on a sample of 10% of the data. We can see from our original linear regression run there was a very slight decrease, around 1.0 decrease in MAE. If our cluster permitted us to run this against our full dataset without any issues, we would hope to run similar tests on the full dataset along with testing different classification models against regression models."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b8ad40de-63e2-496f-94d1-3b403a9415bd","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Results and Discussion\n\nHere we will dicsuss the overall results of the modeling phase of the project. We ran a multitude of experiments when determining the best model to build. We conducted a grid search of different hyperparameters with different sampling methods at various sample sizes to optimize our models in the most efficient way possible. We started by testing many parameters with a very small subset (0.01%) of the data to filter out parameters that perform the worst. We then moved to testing with 1% and 10% and finally to the full dataset to narrow in on which parameters and sampling methods perform the best.\n\nAfter conducting our original experimentation, we built a model with cross-validation with the best parameters from the grid search. We conducted the majority of our testing on the classification models since that is our primary goal of the project. We evaluated various metrics to determine that the best model pipeline is a decision classification model into a linear regression model."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"70c63232-07f3-40ae-a5bc-2cf66bd50cdd","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Top Models\n\n\n#### Classification\nBased on the cross-validation experimentation we determined that a decision tree classification model into a linear regression model would be our choices for a complete pipeline. These models had the best performance on this dataset, although in a later phase we would need to train the model on the full dataset. The decision tree model was chosen as the best model because we saw similar accuracy, precision, recall, and F1, and AUC metrics across all of the classification models (logistic regression, decision tree, random forst, and neural network) but when evaluating the confusion matrix we noticed that many of the models only have good performance because the dataset is imbalanced and many of the models are predicting very few delays. Decision tree had the closest proportion to true delays vs non-delays. Additionally, decision tree had the best accuracy and F1 score, although it did have a lower AUC metric. Our hypothesis is that if we train decision tree on the full dataset, we would be able to improve the AUC. \n\nResults\n$$ \\text{Accuracy: }  0.7747 $$\n$$ \\text{Precision: } 0.8384 $$\n$$ \\text{Recall: } 0.9739 $$\n$$ \\text{F1 Score: } 0.7747 $$\n$$ \\text{AUC: } 0.5956 $$\n\n\nConfusion Matrix\n|                | Positive Prediction | Negative Prediction | \n| -------------- | ------------------- | ------------------- | \n| Positive Label |        108723       |      913738         |\n| Negative Label |        126899       |     4741187         |\n\n\n\nWe also plotted the feature importance and the top features were the percentage of delays in the last 1 day and the 6-6:59 time block, with the percentage of delays in the past day being the most predictive feature by far. This can be seen in the plot of the tree (feature 43 is perc_delays_last1d).\n<img src=\"https://raw.githubusercontent.com/brianahart/spark_flight_predictions/main/Screenshot%202022-12-05%20at%207.27.59%20AM.png\" width=\"800px\">\n\n\nFinally, we looked into when the model works best and where it fails. The details are included at the bottom of [this notebook](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/1215577238253368/command/1215577238259148). We looked into the performance of the model across various categories. We started by looking at the predictions breakdown by type of airport. There are no discernable trends between the type of airport and whether the model was accurate or not. We then looked into the accuracy of predictions by delay reason. Our model was worst at predicting weather delay and security delay and best at predicting carrier delay. This makes sense since percentage of delays in the last day is the most predictive feature and if more flights are delayed coming in it would likely be attributed to carrier delay. The model performs best when there is no delay, which is also expected since that is the base case. We then looked into the predictions by state. When considering origin state, our model was least able to predict delays out of Delaware and Maryland. When considering destination state, our model was least able to predict flights into Puerto Rice. Lastly, we looked into flight delays by month and by day of week. Our model performed worst in predicting delays in July, and in general performed worse on summer months and in December. There was no differnce between performance across the days of the week. \n\n#### Regression\nA regression model was a secondary task, which we would want to expand upon in later iterations of this project. As of now, we have only ran a linear regression model on all of the data. We would want to update this so that the classification model feeds into the regression model so that we are only predicting delay times if flights are going to be delayed and we would want to test more models. \n\nThe results of our linear regression model for this phase of the project are included below:\n\n$$ \\text{R}^2:  0.03364 $$\n$$ \\text{RMSE: } 45.7001 \\text{ min} $$\n$$ \\text{MSE: } 2088.498 \\text{ min}^2 $$\n$$ \\text{MAE: } 18.0777 \\text{ min} $$\n\nThe results of our linear regression model are not ideal, as predicting a delay with +/- 18 minutes does not provide enough specificity for the airline business. This is a top priority for future phases of work, but in the model we currently have, it seems that the most predictive features tended to be the timeblocks of flights leaving between 6-7 am, 7-8am, and midnight to 6am; certain origin and destination airports; and the percentage of whether a flight was delayed in the past 24 hours. \n\n\n*Holiday and Covid-19 Feature Engineered Variables*\nLooking at both the top classification and regression models, it seems that our extra features on holidays and covid-19 did not help our model as much. For holidays perhaps this could be because airports already have many contingency plans to deal with increased traffic around then. And for COVID-19, it is possible that because COVID-19 started in 2020, and we were training from 2015-2019 with validation as 2020 in the experimentation phase to inform our final model decisions, we don't think it was likely that COVID-19 feature helped in training as before January 17th, 2020 its value is zero. It would be interesting to see in the future 2-3 years from now if this changes. \n\n*Tail Number Flights Delayed Feature Engineered Variables*\nThe percentage of flights delayed in the past 1 day tracks an individual tail number and calculates the percentage of legs that plane has flown that were delayed. This feature was highly predictive across all models, indicating that it is very closely tied to delays. This makes sense because if a plane has been delayed a large percentage of its legs in the past 24 hours, it is highly likely that the current flight will also be delayed. For future work, we would want to add an additional variable that looked at percentage of delays in a larger time frame to see if certain planes have any propensity to be delayed. It would also be interesting to do a similar analysis for particular legs."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e022f82e-e67e-480c-9681-372fe543a981","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Gap Analysis\n\nComparing results across other groups for Phase 4, starting with join times we see that our ~12 minute join time is favorable compared to other teams which have join times as high as a few hours. Some groups managed to optimize their dataset down to a join time of ~5 minutes. These teams have slightly smaller datasets so we cant entirely attribute it to dataset size but perhaps other factors. Comparing out training dataset size of ~42M shows that it is relatively in line with the other groups with most datasets clustered around that size but the range is from 20M to 46M. \n\nLooking to training times, we are in line with the other groups but the range is pretty high with some groups completing their joins in ~5 minutes and some taking  several hours. Additional detail is required to further evaluate but our times seem satisfactory. In terms of features, we have the highest amout of features at a total count of 108. This is primarily because we have weather features for both the origin and destination because destination weather can impact delays as well (especially for shorter flights). If we exclude the destination weather variables and some of our feature engineered variables variables, we would be in line with other groups. For most important features, we have similar critical features such as prior departure delays and weather variables as other groups so this is not surprising. \n\nFor model performance, comparing to groups that also used F1 scores for classification models, we had one of the highest at ~77%, many teams scored lower. This could perhaps be attributed to our feature engineering, specifically the C19 variable which accounts for the volatile 2020 period in terms of flight delays and cancellations and time based features as well. One team had a slightly higher F1 score of ~76% and though much color isn't provided on their model, their most important features are time based to predict delays based on weather at a future date in time. Because we implemented similar features, this could be a good rational for the similar F1 score. Any minute difference can be attributed other features and model parameters. It doesn't seem as though other teams have a similar feature based on the 'most important features' column. Turning our attention to the regression model, we were the only team to leverage MAE as a performance metric which is interesting and this could be for a variety of reasons. Perhaps other teams filtered out outliers, or they felt the complex ML models were able to handle the outliers, or that RMSE or MSE was sufficient enough. There perhaps might be several other reasons however we are not able to speculate with limited to no information."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"93d46da1-addd-44c4-972a-14b2ee5c449e","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Conclusion\n\nOur goal for this project was to predict whether a flight in the United States will be delayed, and if so by how many minutes. Flight delays can cause a lot of stress for both passengers and airport staff. But for airport staff especially, having some insight as to whether a flight delay may occur, and by how much can help them better optimize how they respond to flight delays to ensure flights operations run as smoothly as possible and provide better customer care to passengers. We believe that models that incorporate information on basic flight information, aircraft history, weather conditions, holidays, and COVID-19 can predict this. \n\nUsing data on flights in the United States, weather, weather stations near airports, and airport location data, developed a working data and modeling pipeline to develop our classification and regression models. The data pipeline focused on building reading in the data, extracting relevant information, doing necessary cleaning, joining all our datasets together, and final cleaning and feature engineering. We developed features to take into account aircraft history, consider holidays, and the Covid-19 pandemic. Of these added features, family of features on aircraft history - specifically percentage of flights delayed in teh paast 24 hours - played an important role in predicting whether a flight would be delayed or not. For the regression model, this feature also had high predictive power. \n\nWe conducted over 700 experiments total on 4 different classification models (logistic regression, decision tree, random forest, and multi-layer perceptron), and 4 different regression models (linear regression, decision tree, random forest, and gradient boosted trees). This phase of experimentaion was done with 0.1%, 10%, and the full data set without cross validation on years 2015-2020. Only the top performing models were tested with cross validation using 2015-2020 for training and 2021 to test. \n\nIn the end the top classification model was a decision tree model with an F1 score of 0.7746 and accuracy of 0.7746. It's precision and recall were the following respectively: 0.8384 and 0.9739. The relatively high precision and recall indicated to us that not only could our model correctly classify if a flight would be delayed or not, but also with relatively high confidence - both of which are important to have in a situation where our key stakeholders, airport staff, need to make many decisions based on the result of this model. It looked like the most important features were percentage of delays in the last 1 day and the 6-6:59 time block. And our model seemed to perform better at predicting carrier delays and could be better at predicting weather and security delays. \n\nThe top regression model was linear regression which had an MAE of 18.0776. This MAE roughly translates to the notion that our regression model can predict the number of minutes a flight might be delayed +/- 18 minutes. This margin is too large to be implemented in production, and definitely more work needs to be done in improving the quality of the model. It seemed that the most predictive features tended to be the timeblocks of flights leaving between 6-7 am, 7-8am, and midnight to 6am; certain origin and destination airports; and the percentage of whether a flight was delayed in the past 24 hours. \n\nCompared to other groups it seems like our models tended to fair well; our classification f1 score was one of the highest. But we didn't have other groups using MAE to compare our regression results with. But regarding our times for joins we had a relatively faster time for the dataset size we had. Comparing the model running time for experiments and testing was difficult given the many issues our group and other had in getting enough worker nodes to run models in a feasible time and that it kept fluctuating. \n\n*Key Lessons Learned:*\n\nThroughout the different phases there were various key lessons learned that we wanted to highlight. \n\n*Cleaning Strategy Prior to Executing Join* <br/>\nFor example, in phase II where one of the main goals was to execute the join of multiple datasets, we learned valuable lessons on planning our cleaning in relation to executing joins with big data. We learned that the data cleaning should be focused on the columns that will be needed in the join process. This means that having a clear plan on conducting the join(s) is key to ensure we do the essential cleaning to get a successful join. In particular we learned that writing code that goes through the data line by line was very time consuming, so we had to optimize our code as such. This is one of the main reasons we pushed a lot of our deep data cleaning to after the join - to avoid tasks that go line by line. In our first join attempt it took 24+ hours for the join to run until we aborted it. In our final join execution we were able to optimize our code and pipeline to run and save our cleaned data sets in a little over an hour (~6 minutes airlines dataset, ~57 minutes weather dataset, final join saved to parquet in ~11 minutes). \n\n*Incorporating Context in Feature Engineering* <br/>\nDuring phase III, we also realized we had to carefully consider the context in developing extra features. For example, we had originally developed features to get the percentage delayed flights in the 3 months (90 days) but then realized during experimentation that that feature may have some stale data and a smaller time window may be more informative. Thus we changed it to be in the past 24 hours which proved to be of big help in improving our model performance as explained in previous sections.  \n\n*Feature Selection in Tree Based Models* <br/> \nEven though we had a high hunch that our top performing models would be tree based, and that we would not need to worry too much on choosing the important features, we did realize that the selection of features the tree models has to choose from can make a difference. In our earlier phases of experimentation, we had included the count of flights delayed, the number of flights delayed in the past 24 hours and the percentage of flights delayed in the past 24 hours by tail number (i.e. each physical plane). We realized this in our early experimentation stages, where a random forest model was giving greater importance to both percentage and number of flights delayed in the past 24 hours, both of which are probably giving similar information. Given that we had limited how big each tree could be, it seemed as if because these features that are very likely giving similar information, it was more difficult for the random forest to look into pulling in information from other highly predictive features (but relatively less predictive to these). <br/>\n<img src=\"https://raw.githubusercontent.com/brianahart/spark_flight_predictions/main/Screenshot%202022-12-01%20at%201.45.41%20PM%20(1).png\" width=\"600px\">\n\n*Experimenting with Sample Data vs. Full Dataset* <br/>\nAs our dataset was quite large, our original plan was to randomly sample 0.1% then 10% of the data (2015-2019 train; 2020 validate). But we quickly realized that as we scaled up the sample size, our model performance would decay very quickly. After much investigation, we think it might be because the samples we are getting at the different sample sizes may be very different from one another - either in that they may have had unbalanced number of records from different years. Because we applied undersampling to help combat the fact that the dataset was imbalanced, we know that there was an equal number of records from each class for the model to train on. While we complemented our sample tests with tests on the full dataset, it would be interesting to better understand how we can properly iterate with smaller samples in time sensiteive situations. In the future one method to combat this could be do a random stratified sampling (perhaps on year; more investigation would need to be done to determine how) to help ensure that the samples at the different sample sizes would be more comparable to one another. \n\n*Classification Evaluation Metrics in Experimentation* \nWhen doing grid search we primarily focused on accuracy, precision, recall, and f1 scores to evaluate classification models. During the grid search phase of the modeling, we were not plotting a confusion matrix or calculating AUC for every model tested which we know now that we should have. Because of the imbalanced dataset we were getting high precision, recall, and f1 scores just by predicting that amajority of flights would not be delayed. This gave us false confidence in the performance of the models as we moved into the cross validation phase of modeling. Once we started plotting confusion matrices and looking at AUC, we saw that we maybe should have focused on slightly different metrics. Out models with the highest recall, accuracy, and precision often just predicted a small number of delayed flights. AUC is a very key metric that we should have looked at when doing grid search because it shows how much better our model is doing than a random assignment would do. \n\n*Moving Forward:* <br/>\nAt this time, we do not believe that our model is ready for production just yet. There are a few things we would like to explore in the future to further improve what we have developed. They have to do with handling our nulls and investigating further on the cancellation context to bring it back into our classification model. \n\nWhen speaking about the null imputations, specifically we are talking about the remaining nulls in the weather data. The weather data to begin with had a lot of nulls. We were able to impute some nulls using a window function to pull the last known value in the past 3 hours, but that still left many nulls. We think, in the future it would be worth looking into more sophisticated means of imputing the nulls in the weather dataset. Specifically using regression methods for the numeric weather data and either classification or clustering for the weather sky condition categorical variables. This way we can fill in more nulls with more meaningful values. We realize from personal experience, the news, and the data itself that weather can be a predictive factor of a flight delay, and we feel it would be worth the effort to at least explore this avenue to see if that would be true in an improved classification and regression model; especially given that our model is not strong in predicting weather delays. \n\nAnother task we would do would be to look further into predicting cancellations. Specifically on how might differentiate between a delay and a cancellation. We realized in the experimentation phase that the current dataset may not not have enough information to differentiate between delay and cancellations. While we did consider making our model to predict whether a filght would be on-time/early or delayed/cancelled, in the end we decided it could work against our current business goals given the current resources. As we do not have enough information to differentiate between the two, we realized that telling airport staff that a flight might be either delayed or cancelled would not be very helpful because the consequences and logistics to handle these two situations can be quite different. As such only when we have more information on the causal mechanisms behind cancellations and the data to help predict them, would it make sense to add it back into our classification model. Then we can move forward with our idea of feeding in predicted delay flights to the regression model. \n\nWhile we do have a relatively strong classification model, it does need significant improvement, as does our regression model. Once we can explore the above areas, we can be a stronger position to deploy it in production."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b2c0c7da-a465-4619-8455-29a29ac03dee","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"HouseSpark_Master","dashboards":[{"elements":[],"guid":"4c0db8ac-6089-42a0-a5bc-b57c7f3e9256","layoutOption":{"stack":true,"grid":true},"version":"DashboardViewV1","nuid":"f2cef45d-5f24-4d86-8dac-7211a9028b47","origId":2647101326252058,"title":"Untitled","width":1024,"globalVars":{}},{"elements":[],"guid":"de45a698-f050-4ecd-8cc5-722e33f6743e","layoutOption":{"stack":true,"grid":true},"version":"DashboardViewV1","nuid":"7114a47d-4d33-41d9-8389-66308ef55cf1","origId":2647101326252059,"title":"Untitled","width":1024,"globalVars":{}}],"notebookMetadata":{"pythonIndentUnit":2,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":4423519682322132,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":2647101326252034}},"nbformat":4,"nbformat_minor":0}
